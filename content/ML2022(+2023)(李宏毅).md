# TODO
- [ ] Overfitting数学原理
- [ ] 为什么要分验证集
# Intro
- 机器学习--找一个函数  
- 深度学习--类神经网络
向量输入，矩阵(图片)输入，序列(语音...)输入  
几大类别
- 回归输出数值，
- 分类输出类别(选项)，
- structured learning
	- 产生有结构的文件（图片，文档）
- ......
监督非监督怎么监督怎么非监督都是方法  

**鬼哉机器学习**  

**2022年课程涵盖：**
- 自监督学习(self-surpervised learning)-- Pre-train(pre-trained model/foudation model)
  没有标注的数据能直接学习到基本知识，然后神奇的可以用于下游任务(downstream task)  
	- BERT -- pre-trained model 
- unsupervised 非监督学习
	- GAN(Generative Adersarial Network)
	  无需匹配x,y，能自动辨别
- reinforcement learning RL  强化学习
  根据是否成功来判断
- Anomaly detection 异常检测
- Explainable AI  可解释性
  能解释给你
- model attack 模型攻击
- domian adaptation  
- network compression  模型压缩
- life-long learning  
- Meta learning 元学习
	- few-shot learning

### 机器学习基本
#### 流程 
- 找函数--训练   
	1. 写出一个未知参数函数
		1. 需要一些猜测---domain knowledge
	2. 定义损失  loss function
		1. 某一次的参数作为输入
		2. 如果是监督
			1. 计算和数据的差别(差别有多种定义方法)
			2. 一般是所有xy都算一遍
		3. Error surface
			1. 损失的等高线图
	3. 优化 Optimization
		1. Gradient Descent 梯度下降
			1. 随机选一组未知参数数值或者某种方法选出
			2. $g = \nabla L(\boldsymbol{\theta^i})$
			3. 微分偏微分
			4. 根据梯度选择修改方向
			5. 正的参数减少，负的参数增加，刚好相反，实现时是 `-` 
			6. 可能导致局部最小local minimum(只要平了就没有梯度) --- 但是实际上这个问题不会太影响
		2. learning rate  -- hyperparameters
- 预测  
	没有看过的数据  
- 分析  
  做优化
	- 考虑更多的特征作为输入
	- 考虑模型优化 （Model Bias 模型的偏差）--- **更加弹性的model**
	- 优化后，可以加深网络(加层次)   <--- **鬼哉机器学习**
		- 例子是每一次激活函数得到的数值可以再过一遍激活函数

##### 一些概念
- **hyperparameters**(自己设的数值)  
- **Activation function** 激活函数
	- 神经元上的函数，计算每个结点的输出   
	- sigmoid 或 ReLU.....

>[!question]
>理论上，只要足够多的激活函数相加就能逼近任一曲线，在这个意义上深层像是只是把多个激活函数相加变成层次而已，和宽没有太大区别(都叠放在一层)
>**深的意义究竟在哪**？  

- **Overfitting**(过拟合)
- 在训练集中变好，在测试集上变差

课程讲得真的很好，看下来特别舒服，老师使用实例的方式讲解(不太传统)，有些重要概念是直觉性的理解，最后老师做了一些补充  
- [[Backpropagation]]   <--- 极其重要的概念
### 上课时的一些点  
##### 线性回归 （fully-connected）
- 线性模型linear model，无论几元，尽管图像很奇怪，相对于任一变量都是直线，也就是说，此时认为各个特征和目标之间存在的是线性的关系  
  **多元改变的是维度而不是关系**  
	- 当增加特征数而没有改变关系时，关系本身出错结果可能不会有好的改进

##### 一个弹性的模型
- 任意折线/分段曲线 piecewise linear curves都可以由例子中的 _水平线+斜线(斜率)+水平线  + 常数(位置)_ 组成  
	- 推论就会变成，曲线看成稠密的折线，都可以这么组成  
	- ![[Pasted image 20240623102553.png|300]]
	- ![[Pasted image 20240623092854.png|300]]
	- 那这个 _水平线+斜线(斜率)+水平线_ 该怎么表示---**hard sigmoid**
		- [[sigmoid]]  sigmoid是光滑的，用sigmoid 来逼近它
		- 也有其他方法 ---  [[ReLU(Rectified Linear Unit)]]
		- ![[Pasted image 20240623102628.png|325]]
	- 将位置抽出来作为整体的bias  
	- sigmoid方法
		- $$y = b + \textbf{c}^Tsigmoid(\textbf{b} + \textbf{w}\textbf{x})$$
	- ReLU方法
		- $$y = b + \sum_{2i}c_imax(0, b_i+\sum_jw_{ij}x_j)$$
	- 将$b, \textbf{c}, \textbf{b}, \textbf{w}$ 串起来作为参数向量$\boldsymbol{\theta}$  👈**只是一种标记** 
- **实际上使用多少个sigmoid和ReLU来表示曲线并不知道，就练吧大概**  
- 然后每一次激活函数(ReLU或sigmoid)产生的数值在计算预测值$y$前，可以再过几层激活函数  
	- $$y = b + \textbf{c}^T\ \sigma(...\sigma(\textbf{b} + \textbf{w}(\sigma(\textbf{b} + \textbf{w}(\sigma(\textbf{b} + \textbf{wx}))))$$
- 此时变成了 Neural network 神经网络
	- ![[Pasted image 20240623104736.png|400]]
	- 每一层神经元称hidden layer(隐藏层)  --> deep layer --> **Deep learning**

##### 神经元
每一层神经元有一个激活函数，激活函数本身不会过于复杂，接受输入得到输出，完成对目标结果的拟合   
- 每一个输入都是数的加和，只是数的计算方法不同
	- 权重系数加偏移
	- 来自上一层的神经元
##### **batch** 和 **epoch**
- **batch**
- 实际上在计算loss $L$时，不是全部数据都算，而是一个batch一个batch的算
	- **每一个batch拿出来算一遍梯度，更新一遍参数然后下一个**batch  --- 称一次update
	- batch的分法可以有讲究可以没有，一般随便分就行
- **epoch**
	- 所有的batch都看过一遍，称一个epoch
	- 所以**一个epoch更新了多次参数**
- [[#^a219f6|为什么要分batch]]

# What to do  if my network fails to train

#### Training framework  
- step 1 -- define function with unknown parameter 
	- $$y = f_{\theta}(x)$$
- step 2 -- define loss from tranining data
	- $$L(\theta)$$ 
- step 3 -- optimization
	- $$\theta^* = argmin_{\theta}L$$
#### Tips on traninng
![[Pasted image 20240801101326.png]]
- **先检查tranining data loss**，traning data学好了(loss 低)还是没学会(loss 高)
	- **没学会**
	- **model is too simple** 
	  没学会可能是[[model bias]]太小了，模型能够表示的集合中不包含能够很好涵盖训练集的函数
		- **增大模型的弹性**
			- **增加输入的features**
			- **增加深度**
	- **Optimization Issue**
	  模型能够表示的集合中包含能够很好涵盖训练集的函数，但是无法到达
		- **改变optimizer**
	- **如何判断是哪种**
		- **比较模型**（改变层数，改变输入特征数，改变optimizer）
			- **从浅的模型开始**(linear, support vector) 简单的模型一般没有optimization issue
			- **再训练深的model**，比较loss，如果没有更好，那应该是optimization的问题
	- **学会了**
	- 检查loss on testing data
	- **太大**
		- **Overfitting**
		- **training loss 小，testing loss大**
		- 在**弹性很大**的模型上，可能只练出了在训练集上的拟合，在非训练集上很糟糕
		- ![[Pasted image 20240801104452.png]]
		- [[Overfitting数学原理]]
		- **解决**
			- **增加训练资料**，增加对正确分布的拟合程度  👈**一般最有效**
			- **Data augmentation**
				- 根据domain知识，对原数据进行处理，增加数据量或者改进数据
					- 图像 -- 左右翻转，放大缩小
			- **限制模型弹性**  domain knowledge
				- **直接指定函数类型**
				- **less parameters, sharing parameters** -- CNN
				- **less features**
				- **Early stopping**
				- **Regularization**
				- **Dropout**
		- **关于loss与complexity**
		- ![[Pasted image 20240801105638.png]]
			- 过于复杂容易出现overfitting
		- 在overfitting的时候，有可能最终函数恰巧在有限的测试集上表现得很好，但是实际上什么都没学会，仍然是过拟合的状态（只在benchmark corpora上表现好）
			- 将traning set分为traning set 和 validation set，根据validation set结果来判断好坏，能减小拟合到testing set上的可能 👈**为什么**
			- 怎么切分validation set
			- N-fold Cross Validation
				- n等分，取一份当validation set
				- 重复n次，直到每一份都当成过validation set
				- 然后每个模型都在这些分法中训练一遍，取结果平均值最好的模型
				- 然后将这个模型在全部traning set上跑一遍，再进入testing set上测试
		- **Mismatch**
			- **训练资料和测试资料不是同种分布**
			- 怎么解决，见HW11


#### 分batch的原因 ^a219f6






# 公式
MAE-- mean absolute error  
$$e = |y-\hat y|$$ $$L = \frac{1}{N}\sum_ne_n$$
MSE -- mean square error  
$$e = (y-\hat y)^2$$
sigmoid  
$$sigmoid(x) = \frac{1}{1+e^{-x}}$$
$$y = c\frac{1}{1+e^{-(b+wx)}}$$
$$y = c\ sigmoid(b+wx)$$

# 课程网址
- [ML 2022 Spring](https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php)
- [ML 2023 Spring](https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php)

