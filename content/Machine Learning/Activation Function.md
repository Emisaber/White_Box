### å¾…è¡¥
- [ ] Vanishing gradient

# Activation Function
For a given node, the inputs are **multiplied by the weights in a node and summed together**. This value is referred to as the **summed activation of the node**. The summed activation is then transformed via an activation function and defines the specific output or â€œ**activation**â€ of the node.  
The complexity of activation function will increase the complexity of trainning  

## Typical activation function
#### Linear activation function
The simplest activation function is linear function   
$$\sigma  =  cx$$
it is easy to train but cannot learn complex mapping functions  

>Linear activation functions are still used in the output layer for networks that predict a quantity  

### Nonlinear

#### sigmoid
> sigmoid means s-shaped

For a long time, through the early 1990s, it was the default activation used on neural networks.  

sigmoid function, also called the **logistic function**.  
The input to the function is transformed into a value between **0.0 and 1.0**.  

details in [[sigmoid]]
#### hyperbolic tangent

outputs values between -1.0 and 1.0.   
In the later 1990s and through the 2000s, the tanh function was preferred over the sigmoid activation function as models that used it were easier to train and often had better predictive performance.  
better than sigmoid  

details in [[tanh]]  <-  miss
#### Limitation of sigmoid and Tanh

both sigmoid and tanh are **saturate**(é¥±å’Œ)ï¼Œwhich means the large values snap to 1.0 and and small values snap to -1 or 0 for tanh and sigmoid respectively.  
the function only sensitive to changes around theri mid-point(0.5 for sigmoid and 0.0 for tanh) 

é¥±å’Œæƒ…å†µä¸‹ï¼Œåªå¯¹ä¸­é—´å°åŒºé—´çš„æ•°å€¼å˜åŒ–æ•æ„Ÿï¼Œå“ªæ€•è¾“å…¥æ•°å€¼çš„ä¿¡æ¯æ¯”è¾ƒé‡è¦ï¼Œè¾“å‡ºçš„ä¹Ÿæ˜¯ä¸¤ç«¯ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ¢¯åº¦ä¼šåŒæ ·å˜å¾—æžç«¯(0)ï¼Œç¥žç»å…ƒ(æ¿€æ´»å‡½æ•°)æ— æ³•æŽ¥å—æ­£ç¡®çš„æ¢¯åº¦ä¿¡æ¯ï¼Œéš¾ä»¥é€šè¿‡åå‘ä¼ æ’­æ¥æœ‰æ•ˆæ›´æ–°æƒé‡ã€‚  
ReLU ä¿®æ­£äº†è¿™ä¸ªé—®é¢˜ [How to Fix the Vanishing Gradients Problem Using the ReLU - MachineLearningMastery.com](https://machinelearningmastery.com/how-to-fix-vanishing-gradients-using-the-rectified-linear-activation-function/)  ðŸ‘ˆè¿˜æ²¡çœ‹   

#### ReLU
one of piecewise linear hidden units  (åˆ†æ®µçº¿æ€§)
**Rectified(æ•´æµ) linear activation unit  --- ReLU**  
networks that use the rectifier function for the hidden layers are referred to as **rectified networks**.

details in [[ReLU(Rectified Linear Unit)]]


## Reference
- [A Gentle Introduction to the Rectified Linear Unit (ReLU) - MachineLearningMastery.com](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)


