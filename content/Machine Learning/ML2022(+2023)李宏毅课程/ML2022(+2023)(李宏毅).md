# TODO
- [ ] Overfittingæ•°å­¦åŸç†
- [ ] ä¸ºä»€ä¹ˆè¦åˆ†éªŒè¯é›†
# Intro
- æœºå™¨å­¦ä¹ --æ‰¾ä¸€ä¸ªå‡½æ•°  
- æ·±åº¦å­¦ä¹ --ç±»ç¥ç»ç½‘ç»œ
å‘é‡è¾“å…¥ï¼ŒçŸ©é˜µ(å›¾ç‰‡)è¾“å…¥ï¼Œåºåˆ—(è¯­éŸ³...)è¾“å…¥  
å‡ å¤§ç±»åˆ«
- å›å½’è¾“å‡ºæ•°å€¼ï¼Œ
- åˆ†ç±»è¾“å‡ºç±»åˆ«(é€‰é¡¹)ï¼Œ
- structured learning
	- äº§ç”Ÿæœ‰ç»“æ„çš„æ–‡ä»¶ï¼ˆå›¾ç‰‡ï¼Œæ–‡æ¡£ï¼‰
- ......
ç›‘ç£éç›‘ç£æ€ä¹ˆç›‘ç£æ€ä¹ˆéç›‘ç£éƒ½æ˜¯æ–¹æ³•  

**é¬¼å“‰æœºå™¨å­¦ä¹ **  

**2022å¹´è¯¾ç¨‹æ¶µç›–ï¼š**
- è‡ªç›‘ç£å­¦ä¹ (self-surpervised learning)-- Pre-train(pre-trained model/foudation model)
  æ²¡æœ‰æ ‡æ³¨çš„æ•°æ®èƒ½ç›´æ¥å­¦ä¹ åˆ°åŸºæœ¬çŸ¥è¯†ï¼Œç„¶åç¥å¥‡çš„å¯ä»¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡(downstream task)  
	- BERT -- pre-trained model 
- unsupervised éç›‘ç£å­¦ä¹ 
	- GAN(Generative Adersarial Network)
	  æ— éœ€åŒ¹é…x,yï¼Œèƒ½è‡ªåŠ¨è¾¨åˆ«
- reinforcement learning RL  å¼ºåŒ–å­¦ä¹ 
  æ ¹æ®æ˜¯å¦æˆåŠŸæ¥åˆ¤æ–­
- Anomaly detection å¼‚å¸¸æ£€æµ‹
- Explainable AI  å¯è§£é‡Šæ€§
  èƒ½è§£é‡Šç»™ä½ 
- model attack æ¨¡å‹æ”»å‡»
- domian adaptation  
- network compression  æ¨¡å‹å‹ç¼©
- life-long learning  
- Meta learning å…ƒå­¦ä¹ 
	- few-shot learning

### æœºå™¨å­¦ä¹ åŸºæœ¬
#### æµç¨‹ 
- æ‰¾å‡½æ•°--è®­ç»ƒ   
	1. å†™å‡ºä¸€ä¸ªæœªçŸ¥å‚æ•°å‡½æ•°
		1. éœ€è¦ä¸€äº›çŒœæµ‹---domain knowledge
	2. å®šä¹‰æŸå¤±  loss function
		1. æŸä¸€æ¬¡çš„å‚æ•°ä½œä¸ºè¾“å…¥
		2. å¦‚æœæ˜¯ç›‘ç£
			1. è®¡ç®—å’Œæ•°æ®çš„å·®åˆ«(å·®åˆ«æœ‰å¤šç§å®šä¹‰æ–¹æ³•)
			2. ä¸€èˆ¬æ˜¯æ‰€æœ‰xyéƒ½ç®—ä¸€é
		3. Error surface
			1. æŸå¤±çš„ç­‰é«˜çº¿å›¾
	3. ä¼˜åŒ– Optimization
		1. Gradient Descent æ¢¯åº¦ä¸‹é™
			1. éšæœºé€‰ä¸€ç»„æœªçŸ¥å‚æ•°æ•°å€¼æˆ–è€…æŸç§æ–¹æ³•é€‰å‡º
			2. $g = \nabla L(\boldsymbol{\theta^i})$
			3. å¾®åˆ†åå¾®åˆ†
			4. æ ¹æ®æ¢¯åº¦é€‰æ‹©ä¿®æ”¹æ–¹å‘
			5. æ­£çš„å‚æ•°å‡å°‘ï¼Œè´Ÿçš„å‚æ•°å¢åŠ ï¼Œåˆšå¥½ç›¸åï¼Œå®ç°æ—¶æ˜¯ `-` 
			6. å¯èƒ½å¯¼è‡´å±€éƒ¨æœ€å°local minimum(åªè¦å¹³äº†å°±æ²¡æœ‰æ¢¯åº¦) --- ä½†æ˜¯å®é™…ä¸Šè¿™ä¸ªé—®é¢˜ä¸ä¼šå¤ªå½±å“
		2. learning rate  -- hyperparameters
- é¢„æµ‹  
	æ²¡æœ‰çœ‹è¿‡çš„æ•°æ®  
- åˆ†æ  
  åšä¼˜åŒ–
	- è€ƒè™‘æ›´å¤šçš„ç‰¹å¾ä½œä¸ºè¾“å…¥
	- è€ƒè™‘æ¨¡å‹ä¼˜åŒ– ï¼ˆModel Bias æ¨¡å‹çš„åå·®ï¼‰--- **æ›´åŠ å¼¹æ€§çš„model**
	- ä¼˜åŒ–åï¼Œå¯ä»¥åŠ æ·±ç½‘ç»œ(åŠ å±‚æ¬¡)   <--- **é¬¼å“‰æœºå™¨å­¦ä¹ **
		- ä¾‹å­æ˜¯æ¯ä¸€æ¬¡æ¿€æ´»å‡½æ•°å¾—åˆ°çš„æ•°å€¼å¯ä»¥å†è¿‡ä¸€éæ¿€æ´»å‡½æ•°

##### ä¸€äº›æ¦‚å¿µ
- **hyperparameters**(è‡ªå·±è®¾çš„æ•°å€¼)  
- **Activation function** æ¿€æ´»å‡½æ•°
	- ç¥ç»å…ƒä¸Šçš„å‡½æ•°ï¼Œè®¡ç®—æ¯ä¸ªç»“ç‚¹çš„è¾“å‡º   
	- sigmoid æˆ– ReLU.....

>[!question]
>ç†è®ºä¸Šï¼Œåªè¦è¶³å¤Ÿå¤šçš„æ¿€æ´»å‡½æ•°ç›¸åŠ å°±èƒ½é€¼è¿‘ä»»ä¸€æ›²çº¿ï¼Œåœ¨è¿™ä¸ªæ„ä¹‰ä¸Šæ·±å±‚åƒæ˜¯åªæ˜¯æŠŠå¤šä¸ªæ¿€æ´»å‡½æ•°ç›¸åŠ å˜æˆå±‚æ¬¡è€Œå·²ï¼Œå’Œå®½æ²¡æœ‰å¤ªå¤§åŒºåˆ«(éƒ½å æ”¾åœ¨ä¸€å±‚)
>**æ·±çš„æ„ä¹‰ç©¶ç«Ÿåœ¨å“ª**ï¼Ÿ  

- **Overfitting**(è¿‡æ‹Ÿåˆ)
- åœ¨è®­ç»ƒé›†ä¸­å˜å¥½ï¼Œåœ¨æµ‹è¯•é›†ä¸Šå˜å·®

è¯¾ç¨‹è®²å¾—çœŸçš„å¾ˆå¥½ï¼Œçœ‹ä¸‹æ¥ç‰¹åˆ«èˆ’æœï¼Œè€å¸ˆä½¿ç”¨å®ä¾‹çš„æ–¹å¼è®²è§£(ä¸å¤ªä¼ ç»Ÿ)ï¼Œæœ‰äº›é‡è¦æ¦‚å¿µæ˜¯ç›´è§‰æ€§çš„ç†è§£ï¼Œæœ€åè€å¸ˆåšäº†ä¸€äº›è¡¥å……  
- [[Backpropagation]]   <--- æå…¶é‡è¦çš„æ¦‚å¿µ
### ä¸Šè¯¾æ—¶çš„ä¸€äº›ç‚¹  
##### çº¿æ€§å›å½’ ï¼ˆfully-connectedï¼‰
- çº¿æ€§æ¨¡å‹linear modelï¼Œæ— è®ºå‡ å…ƒï¼Œå°½ç®¡å›¾åƒå¾ˆå¥‡æ€ªï¼Œç›¸å¯¹äºä»»ä¸€å˜é‡éƒ½æ˜¯ç›´çº¿ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ­¤æ—¶è®¤ä¸ºå„ä¸ªç‰¹å¾å’Œç›®æ ‡ä¹‹é—´å­˜åœ¨çš„æ˜¯çº¿æ€§çš„å…³ç³»  
  **å¤šå…ƒæ”¹å˜çš„æ˜¯ç»´åº¦è€Œä¸æ˜¯å…³ç³»**  
	- å½“å¢åŠ ç‰¹å¾æ•°è€Œæ²¡æœ‰æ”¹å˜å…³ç³»æ—¶ï¼Œå…³ç³»æœ¬èº«å‡ºé”™ç»“æœå¯èƒ½ä¸ä¼šæœ‰å¥½çš„æ”¹è¿›

##### ä¸€ä¸ªå¼¹æ€§çš„æ¨¡å‹
- ä»»æ„æŠ˜çº¿/åˆ†æ®µæ›²çº¿ piecewise linear curveséƒ½å¯ä»¥ç”±ä¾‹å­ä¸­çš„ _æ°´å¹³çº¿+æ–œçº¿(æ–œç‡)+æ°´å¹³çº¿  + å¸¸æ•°(ä½ç½®)_ ç»„æˆ  
	- æ¨è®ºå°±ä¼šå˜æˆï¼Œæ›²çº¿çœ‹æˆç¨ å¯†çš„æŠ˜çº¿ï¼Œéƒ½å¯ä»¥è¿™ä¹ˆç»„æˆ  
	- ![[Pasted image 20240623102553.png|300]]
	- ![[Pasted image 20240623092854.png|300]]
	- é‚£è¿™ä¸ª _æ°´å¹³çº¿+æ–œçº¿(æ–œç‡)+æ°´å¹³çº¿_ è¯¥æ€ä¹ˆè¡¨ç¤º---**hard sigmoid**
		- [[sigmoid]]  sigmoidæ˜¯å…‰æ»‘çš„ï¼Œç”¨sigmoid æ¥é€¼è¿‘å®ƒ
		- ä¹Ÿæœ‰å…¶ä»–æ–¹æ³• ---  [[ReLU(Rectified Linear Unit)]]
		- ![[Pasted image 20240623102628.png|325]]
	- å°†ä½ç½®æŠ½å‡ºæ¥ä½œä¸ºæ•´ä½“çš„bias  
	- sigmoidæ–¹æ³•
		- $$y = b + \textbf{c}^Tsigmoid(\textbf{b} + \textbf{w}\textbf{x})$$
	- ReLUæ–¹æ³•
		- $$y = b + \sum_{2i}c_imax(0, b_i+\sum_jw_{ij}x_j)$$
	- å°†$b, \textbf{c}, \textbf{b}, \textbf{w}$ ä¸²èµ·æ¥ä½œä¸ºå‚æ•°å‘é‡$\boldsymbol{\theta}$  ğŸ‘ˆ**åªæ˜¯ä¸€ç§æ ‡è®°** 
- **å®é™…ä¸Šä½¿ç”¨å¤šå°‘ä¸ªsigmoidå’ŒReLUæ¥è¡¨ç¤ºæ›²çº¿å¹¶ä¸çŸ¥é“ï¼Œå°±ç»ƒå§å¤§æ¦‚**  
- ç„¶åæ¯ä¸€æ¬¡æ¿€æ´»å‡½æ•°(ReLUæˆ–sigmoid)äº§ç”Ÿçš„æ•°å€¼åœ¨è®¡ç®—é¢„æµ‹å€¼$y$å‰ï¼Œå¯ä»¥å†è¿‡å‡ å±‚æ¿€æ´»å‡½æ•°  
	- $$y = b + \textbf{c}^T\ \sigma(...\sigma(\textbf{b} + \textbf{w}(\sigma(\textbf{b} + \textbf{w}(\sigma(\textbf{b} + \textbf{wx}))))$$
- æ­¤æ—¶å˜æˆäº† Neural network ç¥ç»ç½‘ç»œ
	- ![[Pasted image 20240623104736.png|400]]
	- æ¯ä¸€å±‚ç¥ç»å…ƒç§°hidden layer(éšè—å±‚)  --> deep layer --> **Deep learning**

##### ç¥ç»å…ƒ
æ¯ä¸€å±‚ç¥ç»å…ƒæœ‰ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼Œæ¿€æ´»å‡½æ•°æœ¬èº«ä¸ä¼šè¿‡äºå¤æ‚ï¼Œæ¥å—è¾“å…¥å¾—åˆ°è¾“å‡ºï¼Œå®Œæˆå¯¹ç›®æ ‡ç»“æœçš„æ‹Ÿåˆ   
- æ¯ä¸€ä¸ªè¾“å…¥éƒ½æ˜¯æ•°çš„åŠ å’Œï¼Œåªæ˜¯æ•°çš„è®¡ç®—æ–¹æ³•ä¸åŒ
	- æƒé‡ç³»æ•°åŠ åç§»
	- æ¥è‡ªä¸Šä¸€å±‚çš„ç¥ç»å…ƒ
##### **batch** å’Œ **epoch**
- **batch**
- å®é™…ä¸Šåœ¨è®¡ç®—loss $L$æ—¶ï¼Œä¸æ˜¯å…¨éƒ¨æ•°æ®éƒ½ç®—ï¼Œè€Œæ˜¯ä¸€ä¸ªbatchä¸€ä¸ªbatchçš„ç®—
	- **æ¯ä¸€ä¸ªbatchæ‹¿å‡ºæ¥ç®—ä¸€éæ¢¯åº¦ï¼Œæ›´æ–°ä¸€éå‚æ•°ç„¶åä¸‹ä¸€ä¸ª**batch  --- ç§°ä¸€æ¬¡update
	- batchçš„åˆ†æ³•å¯ä»¥æœ‰è®²ç©¶å¯ä»¥æ²¡æœ‰ï¼Œä¸€èˆ¬éšä¾¿åˆ†å°±è¡Œ
- **epoch**
	- æ‰€æœ‰çš„batchéƒ½çœ‹è¿‡ä¸€éï¼Œç§°ä¸€ä¸ªepoch
	- æ‰€ä»¥**ä¸€ä¸ªepochæ›´æ–°äº†å¤šæ¬¡å‚æ•°**
- [[#^a219f6|ä¸ºä»€ä¹ˆè¦åˆ†batch]]

# What to do  if my network fails to train

#### Training framework  
- step 1 -- define function with unknown parameter 
	- $$y = f_{\theta}(x)$$
- step 2 -- define loss from tranining data
	- $$L(\theta)$$ 
- step 3 -- optimization
	- $$\theta^* = argmin_{\theta}L$$
#### Tips on traninng
![[Pasted image 20240801101326.png]]
- **å…ˆæ£€æŸ¥tranining data loss**ï¼Œtraning dataå­¦å¥½äº†(loss ä½)è¿˜æ˜¯æ²¡å­¦ä¼š(loss é«˜)
	- **æ²¡å­¦ä¼š**
	- **model is too simple** 
	  æ²¡å­¦ä¼šå¯èƒ½æ˜¯[[model bias]]å¤ªå°äº†ï¼Œæ¨¡å‹èƒ½å¤Ÿè¡¨ç¤ºçš„é›†åˆä¸­ä¸åŒ…å«èƒ½å¤Ÿå¾ˆå¥½æ¶µç›–è®­ç»ƒé›†çš„å‡½æ•°
		- **å¢å¤§æ¨¡å‹çš„å¼¹æ€§**
			- **å¢åŠ è¾“å…¥çš„features**
			- **å¢åŠ æ·±åº¦**
	- **Optimization Issue**
	  æ¨¡å‹èƒ½å¤Ÿè¡¨ç¤ºçš„é›†åˆä¸­åŒ…å«èƒ½å¤Ÿå¾ˆå¥½æ¶µç›–è®­ç»ƒé›†çš„å‡½æ•°ï¼Œä½†æ˜¯æ— æ³•åˆ°è¾¾
		- **æ”¹å˜optimizer**
	- **å¦‚ä½•åˆ¤æ–­æ˜¯å“ªç§**
		- **æ¯”è¾ƒæ¨¡å‹**ï¼ˆæ”¹å˜å±‚æ•°ï¼Œæ”¹å˜è¾“å…¥ç‰¹å¾æ•°ï¼Œæ”¹å˜optimizerï¼‰
			- **ä»æµ…çš„æ¨¡å‹å¼€å§‹**(linear, support vector) ç®€å•çš„æ¨¡å‹ä¸€èˆ¬æ²¡æœ‰optimization issue
			- **å†è®­ç»ƒæ·±çš„model**ï¼Œæ¯”è¾ƒlossï¼Œå¦‚æœæ²¡æœ‰æ›´å¥½ï¼Œé‚£åº”è¯¥æ˜¯optimizationçš„é—®é¢˜
	- **å­¦ä¼šäº†**
	- æ£€æŸ¥loss on testing data
	- **å¤ªå¤§**
		- **Overfitting**
		- **training loss å°ï¼Œtesting losså¤§**
		- åœ¨**å¼¹æ€§å¾ˆå¤§**çš„æ¨¡å‹ä¸Šï¼Œå¯èƒ½åªç»ƒå‡ºäº†åœ¨è®­ç»ƒé›†ä¸Šçš„æ‹Ÿåˆï¼Œåœ¨éè®­ç»ƒé›†ä¸Šå¾ˆç³Ÿç³•
		- ![[Pasted image 20240801104452.png]]
		- [[Overfittingæ•°å­¦åŸç†]]
		- **è§£å†³**
			- **å¢åŠ è®­ç»ƒèµ„æ–™**ï¼Œå¢åŠ å¯¹æ­£ç¡®åˆ†å¸ƒçš„æ‹Ÿåˆç¨‹åº¦  ğŸ‘ˆ**ä¸€èˆ¬æœ€æœ‰æ•ˆ**
			- **Data augmentation**
				- æ ¹æ®domainçŸ¥è¯†ï¼Œå¯¹åŸæ•°æ®è¿›è¡Œå¤„ç†ï¼Œå¢åŠ æ•°æ®é‡æˆ–è€…æ”¹è¿›æ•°æ®
					- å›¾åƒ -- å·¦å³ç¿»è½¬ï¼Œæ”¾å¤§ç¼©å°
			- **é™åˆ¶æ¨¡å‹å¼¹æ€§**  domain knowledge
				- **ç›´æ¥æŒ‡å®šå‡½æ•°ç±»å‹**
				- **less parameters, sharing parameters** -- CNN
				- **less features**
				- **Early stopping**
				- **Regularization**
				- **Dropout**
		- **å…³äºlossä¸complexity**
		- ![[Pasted image 20240801105638.png]]
			- è¿‡äºå¤æ‚å®¹æ˜“å‡ºç°overfitting
		- åœ¨overfittingçš„æ—¶å€™ï¼Œæœ‰å¯èƒ½æœ€ç»ˆå‡½æ•°æ°å·§åœ¨æœ‰é™çš„æµ‹è¯•é›†ä¸Šè¡¨ç°å¾—å¾ˆå¥½ï¼Œä½†æ˜¯å®é™…ä¸Šä»€ä¹ˆéƒ½æ²¡å­¦ä¼šï¼Œä»ç„¶æ˜¯è¿‡æ‹Ÿåˆçš„çŠ¶æ€ï¼ˆåªåœ¨benchmark corporaä¸Šè¡¨ç°å¥½ï¼‰
			- å°†traning setåˆ†ä¸ºtraning set å’Œ validation setï¼Œæ ¹æ®validation setç»“æœæ¥åˆ¤æ–­å¥½åï¼Œèƒ½å‡å°æ‹Ÿåˆåˆ°testing setä¸Šçš„å¯èƒ½ ğŸ‘ˆ**ä¸ºä»€ä¹ˆ**
			- æ€ä¹ˆåˆ‡åˆ†validation set
			- N-fold Cross Validation
				- nç­‰åˆ†ï¼Œå–ä¸€ä»½å½“validation set
				- é‡å¤næ¬¡ï¼Œç›´åˆ°æ¯ä¸€ä»½éƒ½å½“æˆè¿‡validation set
				- ç„¶åæ¯ä¸ªæ¨¡å‹éƒ½åœ¨è¿™äº›åˆ†æ³•ä¸­è®­ç»ƒä¸€éï¼Œå–ç»“æœå¹³å‡å€¼æœ€å¥½çš„æ¨¡å‹
				- ç„¶åå°†è¿™ä¸ªæ¨¡å‹åœ¨å…¨éƒ¨traning setä¸Šè·‘ä¸€éï¼Œå†è¿›å…¥testing setä¸Šæµ‹è¯•
		- **Mismatch**
			- **è®­ç»ƒèµ„æ–™å’Œæµ‹è¯•èµ„æ–™ä¸æ˜¯åŒç§åˆ†å¸ƒ**
			- æ€ä¹ˆè§£å†³ï¼Œè§HW11


#### åˆ†batchçš„åŸå›  ^a219f6






# å…¬å¼
MAE-- mean absolute error  
$$e = |y-\hat y|$$ $$L = \frac{1}{N}\sum_ne_n$$
MSE -- mean square error  
$$e = (y-\hat y)^2$$
sigmoid  
$$sigmoid(x) = \frac{1}{1+e^{-x}}$$
$$y = c\frac{1}{1+e^{-(b+wx)}}$$
$$y = c\ sigmoid(b+wx)$$

# è¯¾ç¨‹ç½‘å€
- [ML 2022 Spring](https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php)
- [ML 2023 Spring](https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php)

