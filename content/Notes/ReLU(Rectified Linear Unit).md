
**rectified linear activation funtion or ReLU**    ReLUé€šè¿‡æ­£è´ŸåŒºåˆ†è¾“å…¥  
åŸºæœ¬æ˜¯é»˜è®¤çš„æ¿€æ´»å‡½æ•°  

$$ReLU = max(0,x)$$
è°å¤§é€‰è°  
![[Pasted image 20240623101848.png|425]]


### Limitation of sigmoid and Tanh
- [[sigmoid]]   
- [[Tanh]]
##### Gradient Problems
- [ ] å¾…è¡¥ 

### ReLU Activation

$$ReLU = max(0, x)$$
avoid vanishing/exploding gradient issues(**its gradient is either 0 or 1**) but suffers from the dying ReLU problem  
- **dying ReLU problem -- negative inputs lead to inactive neurons**
	- **Cause**
	  negative input and 0 output, no gradient no update
	- **Impact**
	  Once a ReLU neuron gets stuck in this state where it only outputs zero, it is unlikely to recover
	  è¾“å‡ºæ˜¯0ï¼Œæ¢¯åº¦ä¹Ÿæ˜¯0ï¼Œè¾“å…¥åœ¨è¿™ä¸ªç¥ç»å…ƒå¤„æ¶ˆé™¤äº†ï¼Œè€Œä¸”å› ä¸ºæ¢¯åº¦æ˜¯0æƒé‡ä¸ä¼šä¿®æ”¹ï¼Œæ‰€ä»¥ç¥ç»å…ƒä¸å†æ¿€æ´»(æ­»å»)
	- **Resulting Issues**
	  å¯èƒ½å¯¼è‡´æ— æ³•æ‹Ÿåˆ

```python
if input > 0:
	return input
else:
	return 0
```

![[Pasted image 20240627085242.png|500]]  
è™½ç„¶ä¸æ˜¯å…‰æ»‘çš„ï¼Œä½†æ˜¯å¯ä»¥è®¤ä¸º0å¤„æ–œç‡æ˜¯0  
å®é™…ä½¿ç”¨æ²¡æœ‰é—®é¢˜  
#### advantages
- **Computational Simplicity**
	- `return max(0, x)`  è€Œä¸æ˜¯æŒ‡æ•°è®¡ç®—(sigmoid & tanh)
- **Representational Sparsity**
	- è¿”å›çœŸæ­£çš„0(0.0) è€Œä¸æ˜¯åƒsigmoidå’Œtanhä¸€æ ·è¿”å›è¿‘ä¼¼å€¼   
	-  allowing the activation of hidden layers in neural networks to contain one or more true zero values.  **å…è®¸0**
	- ç§°ä¸º  **ç¨€ç–è¡¨ç¤º(sparse representation)**
		- è¿™ä¸ªç¨€ç–è¡¨ç¤ºåœ¨autoencoderä¸­æ¯”è¾ƒé‡è¦ ğŸ‘ˆä¸æ˜¯å¾ˆæ‡‚ï¼Œè¯¦è§deep learning(èŠ±ä¹¦)
- **Linear Behavior**
	- easy to optimize
	- avoid vanishing gradients
- **Deep networks**
#### tips
- **use ReLU as default activation function**
- **Use with MLPs, CNNs, but probably not RNNs**
	- ä½¿ç”¨ReLUä¹‹åç½‘ç»œè¡¨ç°æš´æ¶¨
	- _The surprising answer is that using a rectifying non-linearity is the single most important factor in improving the performance of a recognition system._
	- When using ReLU with CNNs, they can be used as the activation function on the filter maps themselves, followed then by a pooling layer.
	- ReLU were thought to not be appropriate for Recurrent Neural Networks (RNNs) such as the Long Short-Term Memory Network (LSTM) by default
- **use smaller bias imput value**
	- When using ReLU in your network, consider setting the bias to a small value, such as 0.1
	- èŠ±ä¹¦è¿™ä¹ˆå†™ï¼Œä½†æ˜¯æœ‰äº›äº‰è®®ï¼Œå¯ä»¥éƒ½è¯•è¯•  
- **Use â€œHe Weight Initializationâ€** ğŸ‘ˆ **æ²¡çœ‹æ‡‚**  [PyTorchä¸­çš„Xavierä»¥åŠHeæƒé‡åˆå§‹åŒ–æ–¹æ³•è§£é‡Š\_pytorchä¸­heåˆå§‹åŒ–-CSDNåšå®¢](https://blog.csdn.net/weixin_39653948/article/details/107950764)
	- ä½•æºæ˜ä¹Ÿå¤ªå¼ºäº†
- **Scale Input Data**
	- standardizing variables to have a **zero mean(å‡å€¼ä¸º0)** and **unit variance(æ–¹å·®ä¸º0)** or **normalizing each value to the scale 0-to-1**  
- **Use Weight Penalty**
	- ReLU is unbounded in the positive domain
	- **L1 or L2 vector norm**    <-  ä½¿ç”¨L1æ¯”è¾ƒå¥½
	- 
#### Limitations of ReLU
- **dying ReLU** 
	- å½“å¤§æƒé‡æˆ–è€…å¼‚å¸¸è¾“å…¥æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸ºäº†æ‹Ÿåˆå®é™…è¾“å‡ºï¼Œåç§»biaså˜æˆå¾ˆå¤§çš„è´Ÿæ•°ï¼Œå¯èƒ½å¯¼è‡´æ­£å¸¸çš„è¾“å…¥ä¹Ÿæ˜¯è´Ÿçš„ï¼Œæ­¤æ—¶æ‰€æœ‰è¾“å…¥éƒ½æ˜¯è´Ÿçš„ï¼Œè¾“å‡ºå°±æ˜¯0ï¼Œæ¢¯åº¦åŸåœ°æ¶ˆå¤±



#### Other ReLU
- Leaky ReLU (LReLU or LReL) modifies the function to allow small negative values when the input is less than zero.
- The Exponential Linear Unit, or ELU, is a generalization of the ReLU that uses a parameterized exponential function to transition from the positive to small negative values.
- The Parametric ReLU, or PReLU, learns parameters that control the shape and leaky-ness of the function.
- Maxout is an alternative piecewise linear function that returns the maximum of the inputs, designed to be used in conjunction with the dropout regularization technique.

##### Leaky ReLU
Leaky ReLU introduces a **small gradient for negative inputs**  
é€šè¿‡è°ƒæ•´å­¦ä¹ ç‡å’Œè¯„ä¼°æ¥è°ƒæ•´æ–œç‡    
![[Pasted image 20240625100341.png|350]]
> _è´Ÿæ•°éƒ¨åˆ†æœ‰ä¸€ç‚¹æ–œç‡_

**disadvantege:  Inconsistent output for negative input**  
##### Parametric ReLU(PReLU)

**with learnable slope parameter**  
effectiveness in various applications:  
- computer vision
- speech recognition
éœ€è¦fine-tuneå¾—åˆ°å¯¹åº”çš„è¶…å‚æ•°(learnable parameter)  


##### Gaussian Error Linear Unit (GeLU)
**probabilistic foundations and smooth approximation characteristics**  
GeLU is a **smooth approximation of the rectifier function**, **scaling inputs by their percentile rather than their sign**  
![[Pasted image 20240625102512.png|350]]
- gained notable popularity in transformer architectures  
å…‰æ»‘ä¸”éçº¿æ€§ï¼Œèƒ½å¤Ÿå¾ˆå¥½æ‹Ÿåˆå¤æ‚çš„æ¨¡å‹(CV...)   




## references
- [Understanding ReLU, LeakyReLU, and PReLU: A Comprehensive Guide | by Juan C Olamendy | Medium](https://medium.com/@juanc.olamendy/understanding-relu-leakyrelu-and-prelu-a-comprehensive-guide-20f2775d3d64)
- [Site Unreachable](https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/)
