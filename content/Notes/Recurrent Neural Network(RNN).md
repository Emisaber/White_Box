---
tags:
  - DL
  - ML
---

## Example Application

### Slot filling

å¡«ç©º  

- å¦‚æœä½¿ç”¨Feedforward network(å‰é¦ˆç¥ç»ç½‘ç»œ)
	- Input a word
	- 1-of-N encoding
		- å¯èƒ½éœ€è¦æœ‰otherç»´åº¦æ¥å¤„ç†è¯æ±‡è¡¨ä¸­ä¸å­˜åœ¨çš„è¡¨ç¤º
		- å¯¹è‹±æ–‡å•è¯çš„è¯ï¼Œä½¿ç”¨word hashingï¼Œä¾‹å¦‚a-a-aï¼ˆ$26\times 26 \times 26$ï¼‰
			- apple app, ppl, ple 
	- ä½œä¸ºä¸€ä¸ªåˆ†ç±»ä»»åŠ¡
		- æ¯ä¸€ä¸ªè¯å±äºå„ä¸ªslotçš„æ¦‚ç‡
	- ä½†æ˜¯
	- feedforward networkä¸å­˜åœ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ‰€ä»¥åŒä¸€ä¸ªè¯çš„è¾“å‡ºä¸ä¼šéšç€è¯­æ„å’Œé¡ºåºå‘ç”Ÿæ”¹å˜
		- ![[Pasted image 20240819092816.png]]

## Recurrent Neural Network

### How to keep memory

![[Pasted image 20240819095119.png]]  

å¢åŠ ä¸€ä¸ªè®°å¿†æ¨¡å—ï¼ŒæŠŠè¾“å‡ºå­˜è¿›è®°å¿†é‡Œï¼Œè®°å¿†é‡Œçš„æ•°å€¼å‚ä¸ä¸‹ä¸€æ¬¡è¿ç®—   
å¤§æ¦‚æ˜¯è¿™ä¹ˆä¸ªæµç¨‹æ¥å®ç°è®°å¿†  

è®°å¿†é‡Œé¢çš„æ•°å€¼éšç€è¾“å…¥è¿­ä»£  

![[Pasted image 20240820091110.png]]    
- ä»ç„¶æ˜¯slot filling çš„ä»»åŠ¡ï¼Œæ¯ä¸ªwordè¿›å…¥å¾—åˆ°å®ƒçš„åˆ†ç±»æƒ…å†µ
- åä¸€æ¬¡è¿­ä»£è€ƒè™‘å‰ä¸€æ¬¡çš„å†…å®¹
- å¾ˆæ˜æ˜¾è™½ç„¶ç”±åå¾€å‰æœ‰è®°å¿†æ€§ï¼Œå¯¹äºå…ˆè¾“å…¥çš„è¯æ¥è¯´ï¼Œåè¾“å…¥çš„è¯æ²¡æœ‰è¢«è€ƒè™‘åˆ°(æ²¡æœ‰åæ–‡ä¿¡æ¯)

### RNN

#### Elman Network

Elman Network å°†hidden layerçš„è¾“å‡ºå­˜åˆ°è®°å¿†é‡Œï¼Œå‚ä¸ä¸‹ä¸€æ¬¡è®¡ç®—  
![[Pasted image 20240820091612.png]]  

#### Jordan Network

Jordan Network æŠŠæœ€ç»ˆçš„è¾“å‡ºå­˜è¿›è®°å¿†é‡Œï¼Œå‚ä¸ä¸‹ä¸€æ¬¡è¿ç®—  
ä¼ è¯´æ¯”è¾ƒå¥½  

>å¯ä»¥è§£é‡Šä¸ºï¼Œhiddençš„è¾“å‡ºæ²¡æœ‰targetï¼Œæ¯”è¾ƒéš¾å­¦åˆ°æœ‰ç”¨çš„è®°å¿†ï¼Œæœ€ç»ˆè¾“å‡ºæœ‰targetï¼Œæ¯”è¾ƒå¥½å­¦ï¼Ÿ

![[Pasted image 20240820091635.png]]    

#### Bidirectional RNN

åŒæ—¶trainä¸¤ä¸ªRNNï¼Œæ–¹å‘ä¸åŒï¼Œå°†äºŒè€…çš„è¾“å‡ºç»™ä¸€ä¸ªoutput layerå¾—åˆ°æœ€ç»ˆè¾“å‡º  

è¿™æ ·å°±èƒ½å¤Ÿè€ƒè™‘åˆ°å‰åæ–‡  

![[Pasted image 20240820092304.png]]  

### Long Short-term Memory (LSTM)

é•¿ çŸ­æœŸ è®°å¿†  
ä¸€ç§æ¯”è¾ƒæŒä¹…çš„çŸ­æœŸè®°å¿†æœºåˆ¶  
ä¸€ç§è®°å¿†çš„æœºåˆ¶ï¼Œå››ä¸ªéƒ¨åˆ†  

- Memory cell
	- å­˜å‚¨è®°å¿†
- Input Gate
	- æ˜¯å¦è¾“å…¥åˆ°è®°å¿†é‡Œ
- Output Gate
	- æ˜¯å¦è¾“å‡ºå‚ä¸è®¡ç®—
- Forget Gate
	- æ˜¯å¦å¿˜è®°å½“å‰è®°å¿†

![[Pasted image 20240820092834.png]]   

éœ€è¦æœ‰å››ç§è¾“å…¥æ¥æ§åˆ¶è®°å¿†ï¼Œäº§ç”Ÿä¸€ä¸ªè¾“å‡º  
- æ§åˆ¶ä¸‰ä¸ªé—¨çš„è¾“å…¥
- å¯èƒ½å­˜å…¥è®°å¿†çš„è¾“å…¥
- å¯èƒ½äº§ç”Ÿçš„è¾“å‡º

#### How does it work

![[Pasted image 20240820093820.png]]
$z$ è¡¨ç¤ºè¾“å…¥ï¼Œ$a$ è¡¨ç¤ºè¾“å‡ºï¼Œ$c$ è¡¨ç¤ºå½“å‰çš„è®°å¿†  

Gatesçš„activation functionä¸€èˆ¬æ˜¯sigmoidï¼Œsigmoidè¾“å‡ºåœ¨0-1ä¹‹é—´ï¼Œä»£è¡¨gateæ‰“å¼€çš„ç¨‹åº¦  

##### æµç¨‹

![[Pasted image 20240820100117.png]]  

1. è¾“å…¥ä¸€ä¸ª$z$ï¼Œç»è¿‡activation functionå¾—åˆ°$g(z)$ï¼Œ input gateè¾“å‡º $f(z_i)$ç›¸ä¹˜ï¼Œå¾—åˆ°è¾“å…¥çš„æ–°è®°å¿† $g(z)f(z_i)$  
	1. æ­¤æ—¶$f(z_i)$ä½œä¸ºè¾“å…¥å¼€å…³ï¼Œå¦‚æœ$f(z_i)$ä¸º0ï¼Œåˆ™è¾“å…¥çš„æ–°è®°å¿†ä¸º0
2. $z_f$ è¾“å…¥ç»è¿‡sigmoidå¾—åˆ° $f(z_f)$ï¼Œä¸å½“å‰è®°å¿†$c$ç›¸ä¹˜ï¼Œå¾—åˆ°åŠ æƒçš„å½“å‰è®°å¿† $cf(z_f)$ï¼Œä¸è¾“å…¥çš„æ–°è®°å¿† $g(z)f(z_i)$  åŠ å’Œï¼Œæ›´æ–°è®°å¿†$c$ä¸º$c'$
	1. $$c' = g(z)f(z_i) + cf(z_f)$$
	2. æ­¤æ—¶å¦‚æœ$f(z_f)$ä¸º0ï¼Œåˆ™æ—§è®°å¿†è¢«é—å¿˜
3. è®°å¿†ç»è¿‡activation functionå¾—åˆ° $h(c')$ï¼Œä¸output gateè¾“å‡º $f(z_o)$ ç›¸ä¹˜ï¼Œå¾—åˆ°è¾“å‡º $a$
	1. $$a = h(c')f(z_o)$$
	2. æ­¤æ—¶å¦‚æœ$f(z_o)$æ˜¯0ï¼Œåˆ™æ²¡æœ‰è¾“å‡º

å‡ ä¸ªInputéƒ½æ˜¯ç”±æ¨¡å‹è¾“å…¥ä¸æƒé‡ç›¸ä¹˜å¾—åˆ°çš„åŠ æƒå’Œ  

##### ç»“æ„

LSTMæŠŠmemory cell å½“æˆä¸€ä¸ªneuron  
ä¸€ä¸ªneuronéœ€è¦å››ç»„å‚æ•°  
![[Pasted image 20240820101048.png]]  

å¯¹äºæ•´ä¸ªç½‘ç»œ  
![[Pasted image 20240820101656.png]]  
- memory cell å˜æˆç½‘ç»œçš„neuron
- è¾“å…¥xä¹˜ä»¥ä¸åŒçš„matrixï¼Œå¾—åˆ°å››ç»„è¾“å…¥å‘é‡$z^i,z^f,z^o,z$ï¼Œdimensionä¸è¿™ä¸€å±‚çš„neuronæ•°é‡ä¸€è‡´

å®é™…ä¸Šï¼Œè®¡ç®—ä¸æ˜¯ä»¥memory cellä¸ºå•ä½è¿›è¡Œçš„  

![[Pasted image 20240820102138.png]]  
æ¯ä¸ªè¾“å…¥å‘é‡ï¼Œ$z^i$è¿‡ activation functionåä¸$z$ç›¸ä¹˜ï¼Œ$z^f$ è¿‡activation functionåä¸è®°å¿† $c$ ç›¸ä¹˜ï¼ŒäºŒè€…ç›¸åŠ å¾—åˆ°æ–°è®°å¿†$c'$ï¼Œæ–°è®°å¿†è¿‡activation functionåä¸$z^o$è¿‡activation functionçš„ç»“æœç›¸ä¹˜å¾—åˆ°output $y$  

ç„¶åè¿›å…¥ä¸‹ä¸€è½®  
![[Pasted image 20240820102537.png]]  

å®é™…ä¸Šï¼Œè¾“å…¥å¤„è¿˜ä¼šåŠ å…¥æ–°è®°å¿†(peephole)å’Œä¸Šä¸€æ¬¡çš„è¾“å‡ºï¼ŒæŠŠ$c, y, x$å¹¶åœ¨ä¸€èµ·  

![[Pasted image 20240820102746.png]]  

ç„¶åå å¤šå±‚  
![[Pasted image 20240820102948.png]]  

ç°åœ¨ä½¿ç”¨RNNåŸºæœ¬å°±æ˜¯ç”¨LSTM   

> GRUæ˜¯LSTMçš„ç®€åŒ–ç‰ˆï¼Œå°‘äº†ä¸€ä¸ªgateï¼Œå‚æ•°æ¯”è¾ƒå°‘ï¼Œæ®è¯´è¡¨ç°å·®ä¸å¤š

### Training

å¦‚æœæ˜¯slot fliing  
ä¸€ä¸ªåºåˆ—(å¥å­)ä½œä¸ºè¾“å…¥ï¼Œä¸€ä¸ªwordä¸€ä¸ªlabelï¼Œå¯¹åº”ä¸€ä¸ªreference vectorï¼Œç„¶åè®¡ç®—cross entropy  

è¾“å…¥çš„æœ¬è´¨æ˜¯ä¸€ä¸ªè¯ï¼Œä½†æ˜¯éœ€è¦æŠŠä¸€ä¸ªåºåˆ—å½“æˆä¸€ä¸ªæ•´ä½“ï¼Œé¡ºåºä¸èƒ½æ”¹å˜   

![[Pasted image 20240820104943.png]]  

#### Backpropagation through time(BPTT)

ç›¸æ¯”äºæ™®é€šçš„backpropagationå¤šè€ƒè™‘äº†æ—¶é—´å› ç´   

å¾…è¡¥ğŸ‘ˆ å¯èƒ½è¡¥å§  

#### Problem
##### Rough error surface

![[Pasted image 20240820110928.png]]   

error surface è¦ä¹ˆå¾ˆå¹³å¦è¦ä¹ˆå¾ˆé™¡å³­  
![[Pasted image 20240820111236.png]]  

è§£å†³æ–¹æ³•å¯ä»¥ä½¿ç”¨clipping  
è®¾ç½®é˜ˆå€¼ï¼Œå½“æ¢¯åº¦å¤§äºé˜ˆå€¼æ—¶å°±ç­‰äºé˜ˆå€¼   

ä½†æ˜¯ä¸ºä»€ä¹ˆ  
- [[BPTT]]  
- æˆ–è€…å®é™…ä¾‹å­
- ä¸€ä¸ªç®€å•çš„RNNï¼Œ1å±‚1ä¸ªneuron
- ![[Pasted image 20240820121202.png]]

###### An Example

è¾“å…¥ä¸€ä¸ªé•¿åº¦ä¸º1000çš„åºåˆ—   

![[Pasted image 20240820121202.png]]  

é€šè¿‡ç¨å¾®æ”¹å˜æƒé‡ï¼Œçœ‹çœ‹æ¢¯åº¦æ€ä¹ˆå˜åŒ–   

åœ¨æƒé‡ä¸º1å’Œ1.01æ—¶  **gradient explode**   
![[Pasted image 20240820121628.png]]  

åœ¨æƒé‡ä¸º0.99å’Œ0.01æ—¶  **gradient vanishing**
![[Pasted image 20240820121748.png]]  

ç”±äºåºåˆ—æ¯ä¸ªè¾“å…¥ä¹‹é—´çš„è”ç³»æ˜¯ç´¯ä¹˜çš„ï¼Œæ‰€ä»¥transitionéƒ¨åˆ†å‚æ•°å˜åŒ–çš„å½±å“æ˜¯æŒ‡æ•°çº§çš„  

##### Helpful Techniques

- Long Short-term Memory(LSTM)
	- can deal with gradient vanishing æŠŠå¹³å¦çš„éƒ¨åˆ†æ‹¿æ‰
	- ä¸ºä»€ä¹ˆ
		- memoryå’Œinputæ˜¯ç›¸åŠ çš„
		- forget gateæ˜¯å¯ä»¥å¼€å¯çš„ï¼Œä¿ç•™è®°å¿†çš„å½±å“ (ä¸€èˆ¬biasæ¯”è¾ƒå°ï¼Œç¡®ä¿å¼€å¯)
		- ä½†æ˜¯æ„Ÿè§‰ä¸å¯¹ğŸ‘ˆæŸ¥ä¸€ä¸‹
	- GRU

- Clockwise RNN
- Structually Constrained Recurrent Network (SCRN)

è¿˜æœ‰å¥‡æ€ªçš„ç‚¹  
- ä½¿ç”¨random intializationçš„è¯ï¼Œsigmoid activation function æ¯”è¾ƒå¥½
- ä½¿ç”¨identity matrix çš„è¯ï¼ŒReLUæ¯”è¾ƒå¥½
	- ä¸€èˆ¬çš„RNNå°±å¾ˆå¼º





## Applications

- slot filling (vector sequence as input, each word has a label)
- Input a vector sequence, output a vector (å¤šå¯¹ä¸€)
	- æƒ…æ„Ÿåˆ†æ
		- æœ€åæŠŠhidden layeræ‹¿å‡ºæ¥ï¼Œåšä¸€äº›transformï¼Œåšåˆ†ç±»
	- Key term extraction
- Input a vector sequence, output a vector sequence (å¤šå¯¹å¤š)
	- output shorter
		- speech recognition
			- æ¯ä¸ªvectoræœ‰ä¸€ä¸ªlabelï¼Œç„¶åtrimming(å»é‡)ï¼Œæœ‰çš„æœ¬æ¥å°±æœ‰é‡å¤æ— æ³•è§£å†³
			- Connectionist Temporal Classification (CTC)
				- å…è®¸output nullå­—ç¬¦
				- å¾ˆå¼º
	- ä¸ç¡®å®šé•¿çŸ­ Seq-to-Seq
		- æŠŠåŸè¾“å…¥ç»™RNNæ»šè¿‡ä¸€éï¼Œå­˜ä¸‹è®°å¿†ï¼Œå…ˆäº§ç”Ÿç¬¬ä¸€ä¸ªå­—
		- ç„¶åç¬¬ä¸€ä¸ªå­—ä½œä¸ºè¾“å…¥ï¼Œç»“åˆè®°å¿†ï¼Œäº§ç”Ÿç¬¬äºŒä¸ªå­—ï¼Œä»¥æ­¤ç±»æ¨
		- ![[Pasted image 20240821120912.png|169]]
		- å¦‚ä½•åœæ­¢ --- ç‰¹æ®Šå­—ç¬¦
- Beyond Sequence ï¼ˆstill sequence to sequenceï¼‰
	- Syntactic parsing tree è¯­æ³•åˆ†ææ ‘
		- ![[Pasted image 20240821121437.png|325]]
	- sequnce to sequence auto encoder
		- ç”¨åœ¨æ–‡å­—ä¸Š
			- æŠŠdocument å˜æˆ vector
			- ç”¨RNNæŠŠå¥å­æ»šä¸€éï¼Œç„¶åç»è¿‡decoderå¾—åˆ°åŸå¥å­—(ç±»æ¯”RNN seq-to-seq)çš„åšæ³•
		- ç”¨åœ¨è¯­éŸ³ä¸Š
			- ä¾‹å¦‚è¯­éŸ³æ¯”å¯¹
			- ![[Pasted image 20240821122357.png|300]]
			- audio segment to vectorç”±RNNæ¥åš
				- å£°éŸ³è®¯å·ç”¨RNNæ»šä¸€éï¼Œæœ€åå¾—åˆ°çš„å‘é‡å°±æ˜¯æˆ‘ä»¬éœ€è¦çš„vector
				- è®­ç»ƒçš„æ—¶å€™è¿˜éœ€è¦æœ‰ä¸€ä¸ªdecoderè¿˜åŸå£°éŸ³è®¯å·
				- encoderå’Œdecoderä¸€èµ·train
		- chat bot
			- æ”¶é›†å¯¹è¯
			- encoder-decoder

## Attention-baesd Model

æ³¨æ„åŠ›   
Neural turing machine  
input è¿›æ¥ï¼ŒDNN/RNNæ§åˆ¶reading head controlleråœ¨memoryä¸­è¯»å–ã€‚writting head controlleråŒç†ã€‚  
![[Pasted image 20240822091822.png]]    

### Related problem

#### Reading Comprehension

![[Pasted image 20240822092308.png]]   

æ•´ç¯‡æ–‡ç« è½¬æ¢æˆè¯­ä¹‰çš„vectorï¼Œqueryè¿›æ¥ï¼ŒDNN/RNNé€šè¿‡reading headå†³å®šå“ªä¸ªvectorä¸reading headæœ‰å…³ï¼Œé‡å¤å¤šæ¬¡è¯»å‡ºä¿¡æ¯ã€‚   

#### Visual Question Answering

![[Pasted image 20240822092808.png]]  

DNN/RNN é€šè¿‡reading headé€‰æ‹©è¯»å–å›¾ç‰‡çš„æŸäº›ä½ç½®æ¥è·å–ä¿¡æ¯ã€‚  

#### Speech Question Answering

![[Pasted image 20240822093035.png]]  
å°†é—®é¢˜è½¬åŒ–æˆè¯­ä¹‰çš„vectorï¼Œé€šè¿‡è¯­éŸ³è¾¨è¯†å¾—åˆ°è¯­éŸ³çš„è¯­ä¹‰ã€‚é€šè¿‡reading headé€‰æ‹©è¯»å–è¯­éŸ³è¯­ä¹‰çš„éƒ¨åˆ†å†…å®¹ï¼Œå¾—å‡ºç­”æ¡ˆ  
ç”šè‡³å¯ä»¥é€šè¿‡attentionä¿®æ­£ç­”æ¡ˆï¼Œå°†æœ€ç»ˆç­”æ¡ˆå’Œé€‰é¡¹è®¡ç®—ç›¸ä¼¼åº¦ï¼Œå¾—åˆ°é€‰é¡¹ç­”æ¡ˆ   

è¿™äº›æ˜¯ä¸€èµ·trainçš„  

## RNN & Structured Learning

- RNN LSTM
- HMM,CRF, Structured perception/SVM

![[Pasted image 20240822094044.png]]  

- RNN, LSTM
	- éœ€è¦åŒå‘çš„RNNæ‰èƒ½è€ƒè™‘æ•´ä¸ªsequence
	- costå’Œerrorä¸æ˜¯å¾ˆç›¸å…³
	- ä½†æ˜¯é¬¼å“‰æ·±åº¦å­¦ä¹ 
- structured learning
	- èƒ½æ¯”è¾ƒç®€å•è€ƒè™‘æ•´ä¸ªå¥å­
	- èƒ½æ˜¾å¼çš„å¢åŠ ä¸€äº›é™åˆ¶
	- cost å’Œ error æ˜¯ç›¸å…³çš„
	- ä½†æ˜¯æ•ˆæœä¸å¦‚æ·±åº¦å­¦ä¹ (linear)

**äºŒè€…ç»“åˆï¼Œå°†deepä½œä¸ºå†…éƒ¨ï¼Œdeepçš„è¾“å‡ºä½œä¸ºstructuredçš„è¾“å…¥**  

