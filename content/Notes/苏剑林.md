---
tags:
  - Math
  - ML
  - DL
---
                                                                                
尝试从大佬那里学习  

# Attention is All You Need 浅读
[《Attention is All You Need》浅读（简介+代码） - 科学空间|Scientific Spaces](https://kexue.fm/archives/4765)   

博客从序列编码的角度解读注意力  

### 序列编码对比

序列编码的基本方式是将每个词转化为对应的词向量序列。  
$$X = (x_1, x_2,...,x_t)$$  
每个$x_i$都是一个词对应的词向量，维度为$d$维   
故$$X \in R^{n\times d}$$  
在这里的语义下，应该并不是指序列的输入时的编码，而是指通过某种方式进行编码得到序列信息。   
#### RNN层

$$y_t =f(y_{t-1},x_t)$$  
根据前面的序列与下一个编码  
RNN本质是马尔科夫决策过程，认为这样无法很好地学习到全局地结构信息   
递归的意味着串行的，同时是单向的   
#### CNN层

$$y_t = f(x_{t-1}, x_t, x_{t+1})$$  
使用卷积来实现Seq2Seq的学习，提到值得一读的论文[\[1705.03122\] Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122)    
CNN方便并行，可使用CNN代替RNN   

#### Attention层

RNN需要递归获得信息，CNN只能是在局部卷积，层叠增大感受野  
Attention能够直接获取全局信息   

$$y_t = f(x_t, A, B)$$  
AB都是序列矩阵，A，B应该是K, V，是某个序列矩阵作为Key，某个序列矩阵作为Value计算$x_t$的编码$y_t$  
当$X=A=B$时，为自注意力(self-attention)

![Pasted image 20240920102306](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240920102306.png)   

将$n\times d_k$的序列$Q$编码成了一个新的$n\times d_v$的序列。  

逐个向量看的话  
![Pasted image 20240920102635](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240920102635.png)  

$Z$是归一化因子，和exp对应的都是softmax的运算   
如果是的话，$Z$对应的是e指数的和，exp运算的是$q_t$和$K$中各个$k_s$的内积  
$\sqrt{d_k}$使得softmax的输入不至于过大(否则非0即1，不够soft)   

见[[softmax]]     

#### Multi-Head

使用多个head获取多种注意力编码，然后拼在一起   
![Pasted image 20240920110803](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240920110803.png)  

对QKV使用参数矩阵映射，做h次attention，再拼接起来  
![Pasted image 20240920110916](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240920110916.png)  

### Position Embedding

到这里的注意力计算和位置无关，是计算结果的简单加和，打乱顺序输出是一样的   

论文直接给出了位置编码的公式  
![Pasted image 20240920111355](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240920111355.png)  
称为Sinusoidal形式的Position Embedding 

>_“公式意思是将id为$p$的位置映射为一个$d_{pos}$维的向量，向量的第$i$个元素的数值是$PE_i(p)$  

 
公式应该没有特别的推导，应该是基于知识和实验得到的   
以往更经常用训练得到的向量   
实验效果和训练得到的接近    

#### 几个点  
- Position Embedding本身是绝对位置的信息，但是相对位置信息也很重要，使用三角函数有利于描述相对信息
	- ![Pasted image 20240920112452](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240920112452.png)
- 位置向量和词向量的结合有多种方式，可以直接拼接起来作为一个新向量，也可以把位置向量变得和词向量一样大然后加起来，论文采取的是相加   
- 虽然位置编码这里是$sin$，$cos$的交错形式，但是没有特别的意义，可以任意的重排
	- 如果是拼接重排不影响是显然的
	- 如果是相加，因为向量本身没有局部结构(整体换个序还是一致)，加入的位置向量也就无所谓局部结构，可以任意重排

#### 一些不足之处

- 每个序列两两比较，代价为$O(n^2)$  
- 位置编码并不能很好描述位置信息，可能在一些要求强位置信息的任务上表现不好
- 全局的注意力在一些任务上是不需要的，限定注意力范围可能有用

苏剑林很喜欢CNN可以看得出来，他将注意力理解为CNN的卷积窗口，可能卷积确实有很深的门道  

# 寻求一个光滑的最大值函数

在$x \ge 0$ 且 $y \ge 0$ 的情况下，有最大值函数   
![Pasted image 20240926114008](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926114008.png)  
需要找到一个光滑函数估计max值  

说直观看看不出来，先研究最简单的$f(x) = |x|$   
除了$x = 0$之外，其他可以顺利求导  
![Pasted image 20240926114138](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926114138.png)  

毕竟是有找一个函数拟合，可以发现有这么一个函数(单位阶跃函数)   
![Pasted image 20240926114214](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926114214.png)   

可以认为$f(x)'$    
![Pasted image 20240926124525](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926124525.png)   

此时问题转变为寻找一个$\theta(x)$的近似函数来描述$f(x)$  
有  
![Pasted image 20240926124832](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926124832.png)  
代入$f(x)'$，并积分得到  
![Pasted image 20240926124929](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926124929.png)    

因为在k足够大时，常数项基本没有影响，把它去掉后用于描述$|x|$  
![Pasted image 20240926125035](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926125035.png)   
将式子代入max函数  
![Pasted image 20240926125152](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926125152.png)   
拆开化简   
![Pasted image 20240926125221](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926125221.png)  

因为此时$x \ge 0$ 且 $y \ge 0$，所以将$e^{-2kx}$与$e^{-2ky}$去掉   
![Pasted image 20240926125533](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926125533.png)  
此时的函数等价于  
![Pasted image 20240926125612](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926125612.png)   
到此便基于单位阶跃函数得到一个max函数的近似   

可以发现，此时$x\ge 0$和$y \ge 0$的限制消失了  
同时式子能够推广到多个变量   
![Pasted image 20240926125905](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926125905.png)   

这实际上是，找到一个变化速率大于线性的光滑函数，对二者求和，因为变化速率大于线性，随着参数k的增长更大的数值会突显出来，对数值求和并取反函数得到最大值的估计值。   

类似的有   
![Pasted image 20240926130245](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926130245.png)   
>变化速率不如e（精度低），但是简单

![Pasted image 20240926130257](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020240926130257.png)  
>变化速率大，但是复杂


# 