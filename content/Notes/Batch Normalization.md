---
tags:
  - DL
  - ML
---

training deep neural networks with tens of layers is challenging as they can be sensitive to the **initial random weights** and **configuration of the learning algorithm.**    

one possible reason is the distribution of the inputs to the deep layers may change after each mini-batch when the weights are updated(æ·±å±‚çš„è¾“å…¥åˆ†å¸ƒéšç€æƒé‡æ”¹å˜å‘ç”Ÿå˜åŒ–). This can cause the learning algorithm to forever chase a moving target. (ä¸æ–­æ”¹å˜çš„è¾“å…¥åˆ†å¸ƒéš¾ä»¥æ‰¾åˆ°å’Œç›®æ ‡åˆ†å¸ƒçš„æ­£ç¡®æ˜ å°„)   
the change in the distribution of inputs to layers in the networks is referred to the name "**internal covariate shift(å†…éƒ¨åå˜é‡åç§»)**"(åœ¨æ—¶é—´ä¸Šçš„åç§»è€Œä¸æ˜¯åœ¨ç»´åº¦ä¸Šçš„åç§»)  

> Generally speaking, covariate shift is the case that the changed inputs(the distribution shift around) lead to the need of retraining the network.

Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch.(åœ¨ä¸€ä¸ªbatchå†…è¿›è¡Œå½’ä¸€åŒ–)  

>batchnorm constrain the inputs to keep the same mean and standard deviation at least in a batch

It has the effect of **stabilizing the learning process** and **dramatically reducing the number of training epochs required**.  

### Problem of Training Deep Networks

>_Very deep models involve the composition of several functions or layers. The gradient tells how to update each parameter, under the assumption that the other layers do not change. In practice, we update all of the layers simultaneously._    ---- [Deep Learning](https://amzn.to/2NJW3gE)

ç²—ç•¥åœ°ç†è§£ï¼Œæ¢¯åº¦ä½œä¸ºå‚æ•°æ›´æ–°æ–¹å‘æ˜¯æ²¡æœ‰è€ƒè™‘å…¶ä»–å±‚çš„æ›´æ–°çš„ã€‚æ¢¯åº¦æ›´æ–°å½“å‰å±‚å‡è®¾å‰ä¸€å±‚çš„è¾“å‡ºä¸ä¼šå†å‘ç”Ÿæ”¹å˜ï¼Œè¾“å…¥åˆ†å¸ƒä¸å˜å»æ‹Ÿåˆè¾“å‡ºåˆ†å¸ƒã€‚  

**the update procedure is forever chasing a moving target**  

>_This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities._   ---- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)

about [[Saturating Non-linearities]]

å­˜åœ¨çš„é—®é¢˜ï¼š
1. å‚æ•°çš„æ›´æ–°æ²¡æœ‰è€ƒè™‘å…¶ä»–å±‚çš„æ›´æ–°
2. ç½‘ç»œå†…éƒ¨çš„è¾“å…¥éšæƒé‡æ›´æ–°æ”¹å˜åˆ†å¸ƒ
å®é™…ä¸Šæ˜¯åŒä¸ªé—®é¢˜ï¼Œå‚æ•°æ›´æ–°æ²¡æœ‰åè°ƒï¼Œå„è‡ªè¿›è¡Œå„è‡ªçš„è°ƒæ•´ï¼Œå„å¤„åˆ†å¸ƒä¹Ÿå„è‡ªæ”¹å˜ï¼Œå¯¼è‡´æ— æ³•çœŸæ­£å®ç°å¯¹ç›®æ ‡å‡½æ•°çš„æ‹Ÿåˆã€‚  
### Standardize Layer Inputs

Batch normalization is proposed as a technique to help coordinate the update of mulitiple layers in the model.  

It does this scaling the output of the layer, specifically by **standardizing** the activations of each input variable per mini-batch, such as the activations of a node from the previous layer.(å¯¹æ¯ä¸€å±‚çš„æ¿€æ´»å‡½æ•°è¾“å‡ºè¿›è¡Œæ ‡å‡†åŒ–)  

This process is also called â€œ**whitening**â€ when applied to images in computer vision.  

Batchnorm has the effect of stabilizing and speeding-up the training process of deep neural networks. Especially for CNN and network with sigmoidal nonliearites   

Although **reducing â€œinternal covariate shiftâ€** was a motivation in the development of the method, there is some suggestion that instead batch normalization is effective because it **smooths and, in turn, simplifies the optimization function** that is being solved when training the network.   

### How to standardize  
original standardization is implemented during training by calculating the mean and standard deviation of each input variable to a layer **per mini-batch**  

$$
\mu = \frac{1}{m}\sum_iz^{(i)}
$$  
$$
\sigma^2 = \frac{1}{m}\sum_i(z^{(i)}-\mu)^2
$$  
$$
z_{norm}^{(i)} = \frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\varepsilon}}
$$  
$$
\widetilde{z}^{(i)} = \gamma z_{norm}^{(i)}+\beta
$$  
- Why $\varepsilon$? 
	- It is added for numerical stability and is an arbitrarily small constant. 
- Why $\gamma$ and $\beta$ ?
	- restore the representation power of the network
	- In practice it is common to allow the layer to **learn two new parameters**, namely a new mean and standard deviation, Beta and Gamma respectively, it allow automatic scaling and shifting of the standardized layer inputs. These parameters are learned by the model as part of the training process.  


Given the choice of activation function, the distribution of the inputs to the layer may be quite non-Gaussian. It may be better to standardize the summed activation before activation function.ï¼ˆä¸€äº›æ¿€æ´»å‡½æ•°çš„è¾“å‡ºå¯èƒ½ä¼šéé«˜æ–¯ï¼Œæ­¤æ—¶é€‰æ‹©åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰æ ‡å‡†åŒ–å¯èƒ½æ›´å¥½ï¼‰  
#### Improvement
but if batch size is too small, or mini-batches do not contain a representative distribution of the training set, or differences in the standardized inputs between training and inference can result in differences in performance.   
This can be solved by **Renormalization**.  

>Batch Renormalization extends batchnorm with a per-dimension correction to ensure that the activations match between the training and inference networks.  ---- [Batch Renormalization](https://arxiv.org/abs/1702.03275) 

### Tips for using batch normalization

1. **Use with different network types**
	1. such as MLP, CNN, RNN
2. **Probably use before the Activation**
	1. may be used before or after the activation function in the previous layer
	2. **after** if for s-shaped   -- å’Œæå®æ¯…è€å¸ˆè¯¾ä¸Šè¯´çš„ä¸åŒ
	3. **before** if for activation function that may result in non-Gaussian  --- modern default
	4. **but it depends**
3. **Use Large Learning Rates**
	1. batchnorm smooth the training, so it need much larger learning rates
4. **Less Sensitive to Weight Initializaiton**
5. **Alternate to Data Preparation**
	1. Batchnorm could be used to standardize **raw input variables** that have differing scales.
	2. the batch size must be sufficiently representative of the range of each variable
	3. if variables have highly non-gaussian distribution, it will be better to preform data scaling as a pre-processing step.  å¦‚æœç‰¹å¾çš„åˆ†å¸ƒç›¸å½“éé«˜æ–¯çš„è¯ï¼Œæœ€å¥½åœ¨æ•°æ®é¢„å¤„ç†çš„æ—¶å€™è¿›è¡Œè§„èŒƒåŒ–è€Œä¸æ˜¯åœ¨è®­ç»ƒæ—¶ã€‚
6. **If Use With Dropout**  ğŸ‘ˆ **It depends**
	1. batchnorm offers some regularization effect, reducing generalization error, perhaps no longer requiring dropout for regularization
		1. Each mini-batch is scaled by the mean/variance computed on just that mini-batch
		2. it **adds some noise**(è®¡ç®—å‡ºæ¥çš„å‡å€¼å’Œæ–¹å·®å¹¶ä¸ä»£è¡¨æ•´ä¸ªæ•°æ®é›†) to the values whithin that batch
		3. causes **a slight regularization**
		4. the larger the batch is, the less noise it will has, and the regularization will be reducing as well
	2. random dropout may cause noisy to normalization
7. **At test Time, use the exponatially weighted average across mini-batch as the final mean and standard deviation**  



### References
- [A Gentle Introduction to Batch Normalization for Deep Neural Networks](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)
- [Why Does Batch Norm Work? deeplearning.ai- YouTube](https://www.youtube.com/watch?v=nUUqwaxLnWs)
- [Batch normalization - Wikipedia](https://en.wikipedia.org/wiki/Batch_normalization)
- [Batch Normalization - OpenAI- YouTube](https://www.youtube.com/watch?v=Xogn6veSyxA)  ğŸ‘ˆæ²¡çœ‹
- [\[D\] Batch Normalization before or after ReLU? : r/MachineLearning](https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/) ğŸ‘ˆ see? it is controversial
