---
tags:
  - ML
  - DL
  - LLM
---
## Overview

FlashAttention: åˆ©ç”¨SRAMï¼Œé€šè¿‡æ•°å­¦trickså°†çŸ©é˜µæ‹†åˆ†æˆå—ï¼Œä»¥å‡å°‘HBMè®¿é—®æ¬¡æ•°      

The paper title: FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness    
It means that FlashAttention is:  
- **Fast**
- **Memory-efficient**: compare to $O(N^2)$ of the vanilla attention, FlashAttention is sub-quadratic(æ¬¡äºŒæ¬¡ï¼Œå¢é•¿å°äºäºŒæ¬¡ä½†æ˜¯å¤§äºçº¿æ€§)/linear in $N(O(N))$ 
- **Exact**: Not an approximation(sparse, low-rank).
- **IO aware**: utilize the knowledge of the memory hierarchy

## Preliminaries


[![Pasted image 20250207111845](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020250207111845.png)](https://www.semianalysis.com/p/nvidiaopenaitritonpytorch#%C2%A7the-memory-wall)    

>  Excerpt from the blog: 
>  Over the years GPUs have been adding compute capacity at a faster pace than increasing the memory throughput(TB/s). It doesnâ€™t matter if you can compute at exaFLOPS speeds if there is no data to be processed.   

IO could be the main problem   

Depending on the ratio between computation and memory accesses(commonly measured by the **arithmetic intensity**), operations can be classified as:   
- compute-bound(matrix multiplication)
- memory-bound(elementwise ops(activation, dropout, masking), reduction ops(softmax, layernorm, sum...))   

The computation of attention is **memory-bound**: it is elementwise operations or it has low arithmetic intensity   

One way to tackle memory-bound ops is leverage **the knowledge of memory hierarchy**   

[![Pasted image 20250207114643](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020250207114643.png)](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)

>  GPUs as example   

**the faster the memory, the more expensive it is, and the smaller its capacity**   

A100 GPU has 40â€“80GB of high bandwidth memory (HBM, the thing that gives you lovely CUDA OOMs) with a bandwidth of 1.5â€“2.0 TB/s and 192KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s(108ä¸ªstreaming multiprocessorsæ¯ä¸ªæœ‰192KBçš„on-chip SRAMï¼Œå…±è®¡20MB).  

Since SRAM is far faster than HBM, we could keep some data in SRAM and reduce write and read ops from HBM.   

To exploit the tircks, we should understand how standard attention computes   

[![Pasted image 20250207115228](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020250207115228.png)](https://arxiv.org/pdf/2205.14135)

- vallina attention treats HBM load/store ops as 0 cost.
- write S, read S for softmax, write P and read it again, these IO ops could be **unnecessary/redundant**  
- we could perform all of the intermediate steps in SRAM without redundant IO

The method that keep that keep intermediate result/steps(fusing multiple ops together) in the high speed memory called **kernel fusion**(æ ¸èšå˜)     
[![Pasted image 20250207120122](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020250207120122.png)](https://horace.io/brrr_intro.html)  

>  **A kernel is basically a fancy way of saying â€œa GPU operationâ€**

## FlashAttention

Flash attention boils down(å½’çº³ä¸º) to 2 main ideas  
- Tiling(both forward and backward passes)  --  chunking the $N\times N$ softmax/scores matrix into blocks.   
- Recomputation(backward only)
	- similar to activation/gradient checkpointing

>activation/gradient checkpointing æ˜¯ä¸€ç§ç¼“å­˜éƒ¨åˆ†æ¿€æ´»å€¼(è€Œéå…¨éƒ¨)ä»¥å‡å°‘å†…å­˜ä½¿ç”¨çš„ä¼˜åŒ–æŠ€æœ¯
>å¤§æ¦‚çš„å†…å®¹æ˜¯  
>åœ¨æ¨¡å‹è®­ç»ƒæ—¶ï¼Œé€šå¸¸ä¼šåœ¨å‰ä¼ æ—¶è®¡ç®—å¹¶å­˜å‚¨æ‰€æœ‰å±‚çš„æ¿€æ´»å€¼ï¼Œåœ¨åå‘æ—¶è®¡ç®—æ¢¯åº¦ï¼Œå½“æ¨¡å‹æ·±åº¦å¢åŠ æ—¶ï¼Œè¿™äº›æ¿€æ´»å€¼ä¼šå ç”¨å¤§é‡å†…å­˜  
>activation/gradient checkpointing åªä¿å­˜å…³é”®å±‚çš„æ¿€æ´»å€¼(checkpoints)ï¼Œåœ¨éœ€è¦æ—¶é‡æ–°è®¡ç®—æ¥å¾—åˆ°æ¢¯åº¦ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨çš„åŒæ—¶ä¹Ÿé¢å¤–å¢åŠ äº†è®¡ç®—é‡   


[![Pasted image 20250207174508](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020250207174508.png)](https://arxiv.org/pdf/2205.14135)  

**Step 0:** allocating Q,K,V matrices in HBM  $N$ä¸ªtokenï¼Œ$d$ç»´embeddingï¼ŒSRAMå¤§å°ä¸º$M$ 
**Step 1:**  initialize the block size.   ä¸ºä»€ä¹ˆæ˜¯$\frac{M}{4d}$ ä¸Šå–æ•´ï¼Ÿä¸ºäº†æœ€å¤§åŒ–åœ°åˆ©ç”¨SRAMå¤§å°ï¼ŒSRAMå¤§å°ä¸ºMï¼Œè€Œç®—æ³•æ¯æ¬¡è¿›è¡Œè¿ç®—éœ€è¦ç»´æŠ¤å››ä¸ªblock: åˆ†åˆ«å¯¹åº”q, k, v, o(output)ã€‚æ‰€ä»¥$B_c, B_r$(åˆ—å¤§å°ï¼Œè¡Œå¤§å°)éƒ½ä¸$\frac{M}{4d}$ç›¸å…³ã€‚è€Œä¸ºä»€ä¹ˆ$B_r$æ˜¯$d$ å’Œä¸Šå–æ•´å–minï¼Ÿæ ¹æ®åŸä½œè€…çš„è¯´æ³•ï¼Œè¿™æ˜¯ä¸ºäº†è®©å—çš„å¤§å°ä¸è¶…è¿‡ $M/4$(è§[\[Question\] Does the order of for-loop matter? Â· Issue #766 Â· Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention/issues/766))   

>æœ‰ä¸€äº›æ¯”è¾ƒä¸æ¸…æ¥šçš„ç‚¹å¯èƒ½å¾—ä»å®é™…å®ç°ä¸Šå›ç­”  
>- ä¸ºä»€ä¹ˆæ˜¯ä¸Šå–æ•´ï¼Ÿå¦‚æœæ¯ä¸ªéƒ½ç•¥å¤§äº$M/4d$ï¼Œæœ€ç»ˆä¼šè¶…è¿‡on-chip SRAMçš„å¤§å°ï¼Œæ˜¯å¦æ˜¯å› ä¸ºç•¥è¶…è¿‡å¸¦æ¥çš„communicationå½±å“å¹¶ä¸å¤§æ‰€ä»¥å…è®¸
>- ä¸ºä»€ä¹ˆæ˜¯on-chipï¼Ÿæ˜¯ä¸ºäº†é¿å…SRAMé—´çš„äº¤æµå—ï¼Ÿ

**Step 2:**  initialize $O, l, m$.  $O$ æ˜¯æ³¨æ„åŠ›å±‚çš„æœ€ç»ˆè¾“å‡º(å¤šæ¬¡è¿­ä»£åå¾—åˆ°)ï¼Œ$O$æ˜¯ä¸€ä¸ªçŸ©é˜µ(å¤šä¸ªtokençš„è¾“å‡º)ã€‚$l$ ç»´æŠ¤ç›¸å¯¹äºå—çš„softmax åˆ†æ¯(exp sum)ï¼Œ$m$ç»´æŠ¤å—ä¸­å„ä¸ªtokençš„logitæœ€å¤§å€¼ï¼Œ$l, m$éƒ½æ˜¯å‘é‡ï¼Œç”¨äºè®¡ç®—softmaxã€‚ 

>ä¸ºä»€ä¹ˆSRAMè¿˜èƒ½æ”¾å¾—ä¸‹$l, m$ï¼Œåšå®¢çš„è§£é‡Šæ˜¯register   

**Step 3:**  Divide Q, K, V.  å°†QæŒ‰å—çš„è¡Œå¤§å°è¿›è¡Œæ‹†åˆ†ï¼Œæ‹†åˆ†ä¸º$(B_r, d)$çš„å—($B_r$ä¸ªtokençš„qç»„æˆ)ï¼Œå°†K, VæŒ‰åˆ—å¤§å°è¿›è¡Œæ‹†åˆ†ï¼Œæ‹†åˆ†ä¸º$(B_c, d)$çš„å—($B_c$ä¸ªtokençš„k/vç»„æˆ)ã€‚ä½¿å¾—è®¡ç®—æ³¨æ„åŠ›ä¹‹åï¼Œå½¢æˆ$(B_r, B_c)$å¤§å°çš„å—å­˜å‚¨åœ¨SRAMä¸­   

**Step 4:**  Divide O, l, mï¼Œå‡æ‹†åˆ†ä¸º $T_r$ ä¸ªå—  

**Step 5, 6:** first loop  éå†K, Vã€‚ä»HBMä¸­è½½å…¥ $K_j, V_j$    

**Step 7, 8:** second loop éå†Qã€‚ä»HBMä¸­è½½å…¥ $Q_i, O_i, l_i, m_i$ 

**Step 9:** On chip, compute $S_{ij}$  å³æŒ‰å—è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° $S_{ij} = Q_iK_j^T$ 
 
**Step 10:**  On chip, compute $\tilde m_{ij}$, $\tilde P_{ij}$, $\tilde l_{ij}$  è®¡ç®—æ¯ä¸ªtokenæ³¨æ„åŠ›åˆ†æ•°çš„æœ€å¤§å€¼($m_{ij}$)ï¼Œä¸æ³¨æ„åŠ›åˆ†æ•°ä½œå·®è®¡ç®—exp($P_{ij}$)ï¼Œå¹¶è®¡ç®—expä¹‹å’Œ($l_{ij}$)   

**Step 11:**  On chip, compute $m_i^{new}$, $l_i^{new}$  ä¸ä¸Šä¸€æ¬¡è¿­ä»£ç»´æŠ¤çš„$m_i, l_i$ è®¡ç®—æ–°çš„$m, l$ã€‚å…¶ä¸­må–äºŒè€…æœ€å¤§ï¼Œ$l_{i}^{\mathrm{n e w}}=e^{m_{i}-m_{i}^{\mathrm{n e w}}} l_{i}+e^{\tilde{m}_{i j}-m_{i}^{\mathrm{n e w}}}$   

**Step 12:**  Write new $O_i$ to HBM  è®¡ç®—æ–°çš„$O_i$ åˆ°HBMä¸Š $\mathbf{O}_{i} \gets\mathrm{d i a g} ( \ell_{i}^{\mathrm{n e w}} )^{-1} ( \mathrm{d i a g} ( \ell_{i} ) e^{m_{i}-m_{i}^{\mathrm{n e w}}} \mathbf{O}_{i}+e^{\tilde{m}_{i j}-m_{i}^{\mathrm{n e w}}} \tilde{\mathbf{P}}_{i j} \mathbf{V}_{j} )$  

**Step 13:**  Write new $m, l$ to HBM  

**Step 14,15,16:**  End loop, return $O$  

ç²—ç•¥åœ°ç†è§£ï¼Œè¿™æ ·çš„ç®—æ³•å°†æ³¨æ„åŠ›çš„è®¡ç®—æ‹†åˆ†ä¸ºå—çš„è®¡ç®—ï¼Œä½¿å¾—ä¸HBMçš„äº¤äº’(IOä¸Šçš„æˆæœ¬)å˜å°ï¼Œä¸­é—´è®¡ç®—æ›´é›†ä¸­åœ¨SRAMä¸è®¡ç®—å•å…ƒä¹‹é—´çš„äº¤äº’ï¼ŒåŠ å¿«äº†æ³¨æ„åŠ›çš„è®¡ç®—   

å¤§è‡´çš„æµç¨‹å¯æŒ‰ä¸‹å›¾ç†è§£   
[![Pasted image 20250207174518](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020250207174518.png)](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)    
åˆ—æ–¹å‘çš„æ˜¯ç¬¬äºŒå±‚å¾ªç¯(å³åŸæœ¬ä¸€ä¸ªtokençš„æ³¨æ„åŠ›åˆ†æ•°ç»“æœ)  
è¡Œæ–¹å‘çš„æ˜¯ç¬¬ä¸€å±‚å¾ªç¯  
æœ€ç»ˆå¾—åˆ°çš„æ˜¯æ³¨æ„åŠ›å—çš„è¾“å‡º(ä¸Vç›¸ä¹˜å¹¶ä¸”softmax)   
ç›´åˆ°è¡Œæ–¹å‘çš„ç»“æŸï¼Œå½“å‰tokençš„è¾“å‡ºæ‰æ˜¯æ­£ç¡®å€¼   

æ›´å…·ä½“åœ°   
**è¿™ä¸ªå¥‡æ€ªçš„è®¡ç®—è¿‡ç¨‹(expä½œå·®ï¼Œm, lçš„ç»´æŠ¤ï¼Œä¸ä¸Šä¸€æ¬¡è¿­ä»£ç»“æœçš„è®¡ç®—)æ˜¯æ€ä¹ˆå›äº‹ï¼Ÿ**  
**é¦–å…ˆï¼Œä¸ºä»€ä¹ˆä¼šéœ€è¦è¿™æ ·çš„è¿‡ç¨‹**ï¼Ÿå› ä¸ºsoftmaxã€‚  
softmaxçš„è®¡ç®—ï¼Œéœ€è¦å½“å‰tokenä½œä¸ºqå’Œæ‰€æœ‰dot productçš„ç»“æœã€‚è¿™éœ€è¦æ‰€æœ‰çš„Kéƒ½åŠ å…¥SRAMï¼Œå¾—åˆ°ç»“æœåæ‰èƒ½è¿›è¡Œsoftmaxã€‚ä½†æ˜¯è¿™æ˜¾ç„¶æ˜¯åšä¸åˆ°çš„ï¼ŒSRAMå¹¶ä¸è¶³ä»¥æ”¾ä¸‹æ‰€æœ‰çš„Kå’Œä¸­é—´ç»“æœã€‚è¿™ä¹Ÿæ˜¯åˆ†å—çš„ç›®çš„ï¼Œæ‰€ä»¥ä¸ºäº†å¾—åˆ°ç²¾ç¡®çš„softmaxç»“æœï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›æ•°å­¦æ–¹æ³•   
å¯¹äºä¸€ä¸ªå—ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¸‹é¢çš„è®¡ç®—æ–¹æ³•   
$$
m ( x ) :=\operatorname* {m a x}_{i} \quad x_{i}, \quad f ( x ) :=\left[ e^{x_{1}-m ( x )} \quad\dots\quad e^{x_{B}-m ( x )} \right], \quad\ell( x ) :=\sum_{i} f ( x )_{i}, \quad\mathrm{s o f t m a x} ( x ) :=\frac{f ( x )} {\ell( x )}. 
$$
å…ˆå¿½ç•¥$m(x)$ï¼Œè¿™æ ·çš„è®¡ç®—çº¯ç²¹å°±æ˜¯å¯¹å½“å‰èƒ½å¾—åˆ°çš„logitï¼Œè®¡ç®—ä¸€æ¬¡softmaxè€Œå·²   
å¾ˆå…³é”®çš„ä¸€ç‚¹æ˜¯ï¼Œsoftmaxçš„è®¡ç®—æ˜¯å„ä¸ªlogitçš„expé™¤ä»¥ä¸€ä¸ªå¸¸æ•°ï¼Œæ‰€ä»¥å¯¹äºä¸¤ä¸ªå—çš„è®¡ç®—ç»“æœï¼Œæˆ‘ä»¬åªéœ€è¦æ¶ˆå»å¸¸æ•°çš„å½±å“ï¼Œå†é™¤ä»¥å¸¸æ•°ä¹‹å’Œå°±å¯ä»¥    
$$
m ( x )=m ( \left[ x^{( 1 )} \ x^{( 2 )} \right] )=\operatorname* {m a x} ( m ( x^{( 1 )} ), m ( x^{( 2 )} ) ), \quad f ( x )=\left[ e^{m ( x^{( 1 )} )-m ( x )} f ( x^{( 1 )} ) \quad e^{m ( x^{( 2 )} )-m ( x )} f ( x^{( 2 )} ) \right], 
$$
$$
\ell( x )=\ell( \left[ x^{( 1 )}, x^{( 2 )} \right] )=e^{m ( x^{( 1 )} )-m ( x )} \ell( x^{( 1 )} )+e^{m ( x^{( 2 )} )-m ( x )} \ell( x^{( 2 )} ), \quad\mathrm{s o f t m a x} ( x )=\frac{f ( x )} {\ell( x )}. 
$$   
è¿™å°±è§£é‡Šäº†ç»´æŠ¤ä¸Šä¸€æ¬¡è¿­ä»£ç»“æœçš„åŸå› ï¼Œå¯¹äºæ¯ä¸€æ¬¡ç»“æœï¼Œæˆ‘ä»¬éœ€è¦ç»´æŠ¤$m, l, O_i$æ‰èƒ½æœ€ç»ˆå¾—åˆ°æ­£ç¡®å€¼    
åœ¨å®é™…è¿ç®—ä¸­ï¼Œç®—æ³•å…ˆä¸Vç›¸ä¹˜å†è®¡ç®—äº†softmaxï¼Œä¼˜åŒ–äº†ä¸­é—´è¿‡ç¨‹    
è€Œåœ¨æ¶ˆå»å¸¸æ•°çš„å½±å“ä¸­ï¼Œç®—æ³•ä½¿ç”¨çŸ©é˜µä¹˜æ³•ï¼Œå¯¹äºç»´æŠ¤çš„$l$ï¼Œå–å…¶å¯¹è§’é˜µ$diag(l)$ï¼Œå®é™…ä¸Šå°±æ˜¯å¯¹æ¯ä¸€è¡Œä¹˜ä»¥å¯¹åº”çš„åŠ å’Œ    

**ç„¶åï¼Œä¸ºä»€ä¹ˆéœ€è¦ä½œå·®ï¼Ÿ** ä¸ºäº†æ•°å€¼çš„ç¨³å®šæ€§ã€‚ä½œå·®å¯¹ç»“æœå¹¶æ— å½±å“ï¼Œä½†æ˜¯ä¸ä½œå·®çš„è¯ï¼ŒæŒ‡æ•°è¿ç®—çš„ç»“æœå¯èƒ½è¿‡å¤§ï¼Œå¯¼è‡´æº¢å‡ºæˆ–ç²¾åº¦æŸå¤±ã€‚å‡å»æœ€å¤§å€¼å¯ä»¥é¿å…è¿™ç§ä¸ç¨³å®šæ€§ã€‚è¿™ä¸ªä½œå·®ä¹Ÿé€šè¿‡ä¸­é—´æŒ‡æ•°è¿ç®—è¿›è¡Œæ¶ˆé™¤å’Œå¼•å…¥      

The algorithm can be easily extended to "block-sparse FlashAttention". By doing this we can skip nested for and scale up sequence length  
[![Pasted image 20250209112155](https://raw.githubusercontent.com/Emisaber/pic_obsidian/main/Pasted%20image%2020250209112155.png)](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad)  


### Supplement

##### about complexity

**Space:**  
- standard attention need $O(N^2)$ for space  
- FlashAttention: Q, K, V, O are $(N, d)$ matrices, l, m are $N$ dim vectors. That's $4Nd + 2N$ in total. d usually samller than N, so we get $O(N)$ complexity for space

**Time:**  
- measure by HBM accesses
- standard attention need $\Omega(Nd + N^2)$ 
- FlashAttention access HBM to load blocks. $N/(M/4d)$ blocks and two loops, we get $O(N^2d^2/M^2)$, but we could not load all the blocks at once, each time we load M data. And therefore the result become $O(N^2d^2/M)$  
- for typical number in real world, FlashAttention leads up to 9x fewer accesses

##### about batch size, num_heads, backward pass

**Batch size & num_heads**    
So far the algorithm is basically handled by a single **thread block**. This thread block is executed on a single streaming multiprocessor(SM). To extend the algo on larger scale, we just run $batch\_size \times num\_heads$ threadblocks in parallel on different SMs   

If the number is bigger than the number of available SMs, the CUDA runtime use some sort of queues to implement the logic    

è¿™é‡Œæœ‰ä¸€ä¸ªé—®é¢˜   
å¦‚æœæŒ‰ç°åœ¨çš„åšæ³•ï¼Œå…ˆè¿­ä»£KVï¼Œå†è¿­ä»£Qï¼Œæ¯ä¸€ä¸ªtokençš„è®¡ç®—éƒ½æ˜¯è¢«æ‹†åˆ†çš„ã€‚å¿…é¡»å…ˆè®¡ç®—å‡ºå½“å‰çš„qå¯¹åº”çš„å„ä¸ªkä¸­æœ€å¤§çš„m(1-jä¸ªå—)ï¼Œæ‰èƒ½è®¡ç®—åé¢çš„å—ã€‚ä¹Ÿå°±æ˜¯å“ªæ€•ç°åœ¨åˆ†æˆå¤šä¸ªthread blockï¼Œæ¯ä¸ªSMä¹‹é—´ä¹Ÿå¾—æœ‰äº¤æµ(å…·ä½“æ€ä¹ˆåšå¹¶ä¸æ¸…æ¥š)ã€‚ä¹Ÿå°±æ˜¯è¯´å¿…é¡»ä¸€ä¸ªåºåˆ—ä¸€ä¸ªthread blockã€‚
æœ‰æ²¡æœ‰å¯èƒ½ä¸€ä¸ªåºåˆ—ä¹Ÿèƒ½ä½¿ç”¨å¤šä¸ªthread blockè®¡ç®—å‘¢ï¼Ÿ  
åªéœ€è¦å°†å¾ªç¯é¡ºåºäº¤æ¢å°±å¯ä»¥ï¼Œå…ˆéå†Qï¼Œæ¯ä¸ªtokençš„è®¡ç®—å°±å˜æˆç‹¬ç«‹çš„ã€‚ä¸åŒtokenå°±å¯ä»¥æ”¾åœ¨ä¸åŒthread blockè®¡ç®—ã€‚ä¹Ÿå°±æ˜¯FlashAttention v2ä¸­çš„ä¸€ä¸ªæ”¹è¿›   

**backward pass & recomputation**    

åå‘ä¼ æ’­æ¯”è¾ƒå¤æ‚ã€‚  
æ ‡å‡†çš„attentionä¼šåœ¨å‰ä¼ æ—¶å­˜å‚¨$P$ç”¨äºè®¡ç®—æ¢¯åº¦ï¼ŒPæ˜¯ä¸€ä¸ª$O(N^2)$çš„çŸ©é˜µï¼Œè€ŒFlashAttentionéœ€è¦Så’ŒPè®¡ç®—æ¢¯åº¦ï¼Œåœ¨åå‘ä¼ æ’­æ—¶ä½¿ç”¨QKVé‡æ–°è®¡ç®—S, P(recomputation)  
åŒæ ·è¿›è¡Œtiling  

æ›´å…·ä½“å¾—å‚è€ƒè®ºæ–‡å’Œå…¬å¼æ¨å¯¼    
- [FlashAttention v1ã€v2 - å…¬å¼æ¨å¯¼ && ç®—æ³•è®²è§£](https://zhuanlan.zhihu.com/p/680091531)   
- [FlashAttention åå‘ä¼ æ’­è¿ç®—æ¨å¯¼](https://zhuanlan.zhihu.com/p/631106302)   
- [LLMï¼ˆåä¸ƒï¼‰ï¼šä» FlashAttention åˆ° PagedAttention, å¦‚ä½•è¿›ä¸€æ­¥ä¼˜åŒ– Attention æ€§èƒ½](https://zhuanlan.zhihu.com/p/638468472)

## References

- [ELI5: FlashAttention. Step by step explanation of how one ofâ€¦ \| by Aleksa GordiÄ‡ \| Medium](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad) ğŸ‘ˆ mainly  å¥½æ–‡æ¨è
- [Flash Attention (Fast and Memory-Efficient Exact Attention with IO-Awareness): A Deep Dive \| by Anish Dubey \| Towards Data Science](https://medium.com/towards-data-science/flash-attention-fast-and-memory-efficient-exact-attention-with-io-awareness-a-deep-dive-724af489997b)
- [Flash Attention](https://huggingface.co/docs/text-generation-inference/en/conceptual/flash_attention)
- [\[2205.14135v2\] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135v2)
- [\[2307.08691\] FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
